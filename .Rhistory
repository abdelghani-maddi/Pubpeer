<<<<<<< HEAD
urls <- str_extract_all(text, url_pattern)[[1]]
urls
urls <- str_extract_all(text, "(?<=href=\")[^\"]*")
urls
View(urls)
urls[[1]]
urls <- unlist(regmatches(text, gregexpr("https?://\\S+|www\\.\\S+", gsub("\"", "'", text), perl=TRUE)))
urls[1]
urls
rm(list = ls()) #supprimer tous les objets
# chargement des packages ----
library(stringr)
library(tibble)
library(tidytext)
library(textdata)
library(Hmisc)
library(zoo)
library(flextable)
library(DBI)
library(data.table)
library(tidyverse)
library(trimmer)
library(DescTools)
library(questionr)
library(RPostgres)
library(lubridate)
library(timechange)
library(urltools)
library(stringr)
library(rebus)
library(Matrix)
library(plyr)
library(sjmisc)
library(regexplain)
library(gtsummary)
library(igraph)
# install.packages('remotes')
# remotes::install_github("gadenbuie/regexplain")
# Dossier travail
setwd('/Users/maddi/Documents/Pubpeer project/Pubpeer')
# Connexion ----
con<-dbConnect(RPostgres::Postgres())
db <- 'SKEPTISCIENCE'  #provide the name of your db
host_db <- 'localhost' # server
db_port <- '5433'  # port DBA
db_user <- 'postgres' # nom utilisateur
db_password <- 'Maroua1912'
con <- dbConnect(RPostgres::Postgres(), dbname = db, host=host_db, port=db_port, user=db_user, password=db_password)
# Test connexion
dbListTables(con)
### Récupération des données ----
reqsql= paste('select inner_id, publication, html as comm from data_commentaires')
data_comm = dbGetQuery(con,reqsql)
### Récupération des commentaires avec liens hypertextes ----
# Creer la liste des liens qui se trouvent dans les commentaires gardant
# le lien avec les publications à partir desquelles sont extraits
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%https%","%http%","%www%","%WWW%")) %>% # Etape 1
#### Etape 2 : Recupérer uniquement les commentaires (l'intérêt de garder le subset est de pouvoir selectionner par la suite les disciplines aussi) ----
.$comm # Etape 2
#### Etape 3 : nommer le vecteur avec l'identifiant des publications ----
names(URL_var) <- subset(data_comm$publication, data_comm$comm %like% c("%https%","%http%","%www%","%WWW%")) # Etape 3
#### Etape 4 : Créer un pattern pour l'extraction des URL ----
pat<- "//" %R% capture(one_or_more(char_class(WRD,DOT)))
#### Etape 5 : Utiliser "rebus" pour extraire l'URL principal ----
URL_extract<-str_extract_all(URL_var, "(?<=//)[^\\s/:]+") #URL_extract<-str_match_all(URL_var, pattern = pat)
# Attribuer aux liens, les identifiants des publications d'où ils sont issus ----
names(URL_extract) <- names(URL_var)
# Transformer en liste de dataframe
list_data <- Map(as.data.frame, URL_extract) %>%
rbindlist(., use.names = F, idcol = names(.))
names(list_data) = c("publication", "site")
df = data.frame(list_data$publication, factor(list_data$site))
names(df) = c("publication", "site")
# list_data[list_data$publication == "106541"] -- petit test -- OK :)
# Nettoyer les données et préparer des fichiers txt pour vosviewer ----
`%not_like%` <- purrr::negate(`%like%`) # juste au cas où j'aurais besoin du not_like.
`%not_in%` <- purrr::negate(`%in%`) # juste au cas où j'aurais besoin du not_like.
donnees <- list_data[df$site %like% "%\\.%"]
# Le site doit contenir au moins un caractère alphabetique
donnees1 <- donnees[grep("[a-zA-Z]", donnees$site), ]
# Supprimer tous les caractères spéciaux sauf le "."
donnees2 <- data.frame(donnees1$publication, stringr::str_remove_all(donnees1$site, "[\\p{P}\\p{S}&&[^.]]"))
names(donnees2) = c("publication", "site")
# Transformation des données (sites as integer)
donnees3 <- data.frame(as.integer(donnees2$publication), donnees2$site)
names(donnees3) = c("publication", "site")
## Exportation des données ----
#  dbWriteTable(conn = con, "publication_sites_comm", donnees3)
# Typologie des sites ----
## Transformer en "factor" les sites
t <- data.frame(donnees3$publication, factor(donnees3$site))
names(t) <- c("publication","site")
# Utiliser la fonction ave() pour ajouter une colonne avec une séquence numérique qui se réinitialise selon id
t$seq <- ave(t$publication, t$publication, FUN = function(x) seq_along(x))
# Utiliser la fonction group_by() pour regrouper les données par identifiant
grouped_df <- t %>%
group_by(publication) %>%
mutate(quartile = ntile(seq, 4))
# Calculer les fréquences pour avoir une idée de la distribution des sites
f <- grouped_df$site |>
fct_infreq() |>
questionr::freq()
# juste pour rajouter la noms de lignes en tant que colonne
freqsit <- data.frame(rownames(f),f)
names(freqsit) = c("site","nb","part","freq")
# Importer les données téseaurus pour unifier et mettre en forme les sites (ceux qui sont les plus fréquents)
class_sites <- readxl::read_xlsx("classification sites.xlsx", col_names = TRUE)
t2 <- t %>%
left_join(class_sites, by = c("site")) %>% # un left join avec expressions régulières (contain)
data.frame(factor(.$site), factor(.$type_sit)) %>%
.[,c(1,3,6,7)]
names(t2) <- c("publication", "seq","site", "sit_harm")
# Remplacer les valeurs manquantes dans col1 avec les valeurs correspondantes dans col2
t2$site <- as.character(t2$site)
t2$sit_harm <- as.character(t2$sit_harm)
t2$sit_harm[is.na(t2$sit_harm)] <- t2$site[is.na(t2$sit_harm)]
t2$site <- factor(t2$site)
t2$sit_harm <- factor(t2$sit_harm)
dbWriteTable(con, "data_sites_harmo", t2)
# Typologie des sites ----
## Transformer en "factor" les sites
t <- data.frame(donnees3$publication, factor(donnees3$site))
names(t) <- c("publication","site")
# Utiliser la fonction ave() pour ajouter une colonne avec une séquence numérique qui se réinitialise selon id
t$seq <- ave(t$publication, t$publication, FUN = function(x) seq_along(x))
# Utiliser la fonction group_by() pour regrouper les données par identifiant
grouped_df <- t %>%
group_by(publication) %>%
mutate(quartile = ntile(seq, 4))
# Calculer les fréquences pour avoir une idée de la distribution des sites
f <- grouped_df$site |>
fct_infreq() |>
questionr::freq()
# juste pour rajouter la noms de lignes en tant que colonne
freqsit <- data.frame(rownames(f),f)
names(freqsit) = c("site","nb","part","freq")
# Importer les données téseaurus pour unifier et mettre en forme les sites (ceux qui sont les plus fréquents)
class_sites <- readxl::read_xlsx("classification sites.xlsx", col_names = TRUE)
t2 <- t %>%
left_join(class_sites, by = c("site")) %>% # un left join avec expressions régulières (contain)
data.frame(factor(.$site), factor(.$type_sit)) %>%
.[,c(1,3,6,7)]
names(t2) <- c("publication", "seq","site", "sit_harm")
# Remplacer les valeurs manquantes dans col1 avec les valeurs correspondantes dans col2
t2$site <- as.character(t2$site)
t2$sit_harm <- as.character(t2$sit_harm)
t2$sit_harm[is.na(t2$sit_harm)] <- t2$site[is.na(t2$sit_harm)]
t2$site <- factor(t2$site)
t2$sit_harm <- factor(t2$sit_harm)
#dbWriteTable(con, "data_sites_harmo", t2)
# exclure du dataframe toutes les lignes (sites) dont on sait qu'ils ne sont pas des sites de discussion
# Récupérer les données de sites à exclure
class_sites2 <- readxl::read_xlsx("classification sites2.xlsx", col_names = TRUE)
# Vecteur contenant les valeurs à exclure
search_strings <- class_sites2$site
# Extraction des lignes pour lesquelles la colonne "element" contient une partie des chaînes de caractères
new_df <- t2[!apply(sapply(search_strings, function(x) grepl(x, t2$sit_harm)), 1, any), ]
# Calculer les fréquences pour avoir une idée de la distribution des sites
f <- factor(new_df$sit_harm) |>
fct_infreq() |>
questionr::freq()
f_a_analyser <- data.frame(rownames(f),f)
View(f_a_analyser)
View(data_comm)
View(df)
data_comm$comm[data_comm$publication==1]
View(df)
View(t2)
View(f_a_analyser)
data_comm$comm[data_comm$publication==66853]
View(f_a_analyser)
data_comm$comm[data_comm$publication==1]
View(f_a_analyser)
View(grouped_df)
ids <- seq_along(groups)
# Créer un dataframe à partir de la liste groups et des identifiants
df <- data.frame(id = rep(ids, lengths(groups)),
element = unlist(groups))
# Créer un vecteur d'identifiants pour chaque groupe
ids <- seq_along(groups)
# Afficher les groupes
groups
View(f_a_analyser)
View(f_a_analyser)
View(f_a_analyser)
View(t2)
# Importer les données téseaurus pour unifier et mettre en forme les sites (ceux qui sont les plus fréquents)
class_sites <- readxl::read_xlsx("classification sites.xlsx", col_names = TRUE)
t2 <- t %>%
left_join(class_sites, by = c("site")) %>% # un left join avec expressions régulières (contain)
data.frame(factor(.$site), factor(.$type_sit)) %>%
.[,c(1,3,6,7)]
names(t2) <- c("publication", "seq","site", "sit_harm")
# Remplacer les valeurs manquantes dans col1 avec les valeurs correspondantes dans col2
t2$site <- as.character(t2$site)
t2$sit_harm <- as.character(t2$sit_harm)
t2$sit_harm[is.na(t2$sit_harm)] <- t2$site[is.na(t2$sit_harm)]
t2$site <- factor(t2$site)
t2$sit_harm <- factor(t2$sit_harm)
f2 <- factor(t2$sit_harm) |>
fct_infreq() |>
questionr::freq()
View(f2)
f2 <- factor(t2$sit_harm) |>
fct_infreq() |>
questionr::freq()
f_a_analyser <- data.frame(rownames(f2),f2)
View(f_a_analyser)
f_a_analyser <- data.frame(rownames(f2),f2$n,f$`val%`)
f_a_analyser <- data.frame(rownames(f2),f2$n,f2$`val%`)
names(f_a_analyser) <- c("site", "nbr_apparitions", "part")
View(f_a_analyser)
View(f_a_analyser)
View(f_a_analyser)
library(xlsx)
install.packages("xslx")
library(xlsx)
java -version
install.packages("rJava")
library(xlsx)
library(rJava)
install.packages("xlsx")
library(xlsx)
library(rJava)
library(rJava)
.jinit()
install.packages('rJava')
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
if (Sys.getenv("JAVA_HOME")!="")
Sys.setenv(JAVA_HOME="")
library(rJava)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
library(rJava)
# Créer une nouvelle colonne pour stocker les URL extraites
data_comm$urls <- NA
=======
>>>>>>> c71e55c63cf7d8936fec11d3863d640058fbbafb
nrow(data_comm)
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(data_comm)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(data_comm$comm[i], regexpr("(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", data_comm$comm))
# Stocker l'URL extraite dans la nouvelle colonne
data_comm$urls[i] <- extracted_url
}
library(stringr)
library(dplyr)
# Créer une nouvelle colonne pour stocker les URL extraites
data_comm$urls <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(data_comm)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(data_comm$comm[i], regexpr("(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", data_comm$comm))
# Stocker l'URL extraite dans la nouvelle colonne
data_comm$urls[i] <- extracted_url
}
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(data_comm$comm[i], regexpr("(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", data_comm$comm))
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(data_comm$comm[i], regexpr("(?i)\\b((?:https?://|www\\d{{0,3}}[.]|[a-z0-9.\\-]+[.][a-z]{{2,4}}/)(?:[^\\s()<>]+|(\\([^\\s()<>]+\\)))*+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", data_comm$comm))
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(data_comm)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(data_comm$comm[i], regexpr("(?i)\\b((?:https?://|www\\d{{0,3}}[.]|[a-z0-9.\\-]+[.][a-z]{{2,4}}/)(?:[^\\s()<>]+|(\\([^\\s()<>]+\\)))*+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", data_comm$comm))
# Stocker l'URL extraite dans la nouvelle colonne
data_comm$urls[i] <- extracted_url
}
# Créer un vecteur de test contenant des URL
urls <- c("https://www.example.com/1", "https://www.example.com/2", "https://www.example.com/3")
# Créer une dataframe contenant une colonne de test avec les URL
df <- data.frame(colonne_texte = c("Voici une URL : https://www.example.com/1", "Une autre URL : https://www.example.com/2", "Encore une URL : https://www.example.com/3"))
# Créer une nouvelle colonne pour stocker les URL extraites
df$new_colonne <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(df)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(df$colonne_texte[i], regexpr("(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", df$colonne_texte))
# Stocker l'URL extraite dans la nouvelle colonne
df$new_colonne[i] <- extracted_url
}
# Créer un vecteur de test contenant des URL
urls <- c("https://www.example.com/1", "https://www.example.com/2", "https://www.example.com/3")
# Créer une dataframe contenant une colonne de test avec les URL
df <- data.frame(colonne_texte = c("Voici une URL : https://www.example.com/1", "Une autre URL : https://www.example.com/2", "Encore une URL : https://www.example.com/3"))
# Créer une nouvelle colonne pour stocker les URL extraites
df$new_colonne <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(df)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(df$colonne_texte[i], regexpr("(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", df$colonne_texte))
# Stocker l'URL extraite dans la nouvelle colonne
df$new_colonne[i] <- extracted_url
}
rm(list = ls()) #supprimer tous les objets
<<<<<<< HEAD
library(tidyverse)
library(questionr)
library(gtsummary)
### Récupération des données ----
## importer les données quand elles sont en local et non sur Pstgresql
df <- readxl::read_excel("D:/bdd/data_pub.xlsx")
# revues
jnal <- select(df, starts_with("Journal"))
library(httr)
library(jsonlite)
library(jsonlite)
# Envoyer la requête à l'API openalex
response <- GET("https://api.openalex.org/publications?year=2021&groupby=country")
# Convertir la réponse en JSON
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
# Convertir les données JSON en data frame
df <- as.data.frame(json_data)
View(response)
View(df)
# Envoyer la requête à l'API openalex
response <- GET("https://api.openalex.org/publications?year=2021&groupby=country")
# Convertir la réponse en JSON
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
# Envoyer la requête à l'API openalex
response <- GET("https://api.openalex.org/institutions?group_by=country_code")
# Convertir la réponse en JSON
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
# Convertir les données JSON en data frame
df <- as.data.frame(json_data)
View(response)
response[["status_code"]]
response <- GET("https://api.openalex.org/institutions?group_by=country_code")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
df_institutions <- as.data.frame(json_data)
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/institutions?group_by=country_code")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
df_institutions <- as.data.frame(json_data)
View(json_data)
json_data[["group_by"]]
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/works?group_by=country_code")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
df_institutions <- as.data.frame(json_data)
View(json_data)
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
json_data <- content(response, as = "text")
json_data <- content(response, as = "text")
View(response)
json_data <- content(response, as = "text")
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
json_data <- content(response, as = "text")
View(response)
response[["status_code"]]
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
View(response)
json_data <- content(response, as = "text")
library(tidyverse)
library(questionr)
library(gtsummary)
library(httr)
library(jsonlite)
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
View(response)
response[["status_code"]]
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
json_data <- content(response, as = "text")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
df_publications <- as.data.frame(json_data)
View(json_data)
json_data[["group_by"]]
df_publications <- as.data.frame(json_data)
df_publications <- as.data.frame(json_data$group_by)
View(df_publications)
# Fonction pour récupérer le nombre de publications pour une institution donnée
get_publication_count <- function(institution_id, year) {
url <- paste0("https://api.openalex.org/institutions/", institution_id, "/publications")
response <- GET(url, query = list(year = year))
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
return(json_data$count)
}
# Initialiser un data frame pour stocker les volumes de publication par pays
df_publications <- data.frame(country_code = character(),
publication_count = integer())
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
df_publications <- as.data.frame(json_data$group_by)
View(df_publications)
tbl_summary(df_publications)
View(df_publications)
nb_openalex <- df_publications |>
fct_infreq() |>
questionr::freq()
nb_openalex <- df_publications$count |>
fct_infreq() |>
questionr::freq()
View(df_publications)
## importer les données quand elles sont en local et non sur Pstgresql
df <- readxl::read_excel("D:/bdd/data_pub.xlsx")
## explo rapide
glimpse(df)
summary(df$concepts_mot)
describe(df$concepts_mot)
describe(df$Journal_Categories_WOS)
describe(df$Journal_Domaines_WOS)
describe(df$Journal_Domaines_Scimago)
describe(df$concepts_mot)
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?filter=concepts_count:%3E2")
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?filter=concepts_count:%3E2")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
df_concepts <- as.data.frame(json_data$group_by)
View(json_data)
json_data[["results"]]
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?filter=concepts_count:%3E2")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
View(json_data)
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?filter=concepts_count:%3E0")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
View(json_data)
json_data[["results"]]
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?group_by=concepts.id")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
View(json_data)
json_data[["group_by"]]
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
View(json_data)
df_publications <- as.data.frame(json_data$group_by)
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?group_by=concepts.id")
json_data <- content(response, as = "text")
total_count <- fromJSON(json_data)[[1]]
View(total_count)
# Définir le nombre d'éléments à récupérer par page
limit <- 200
# Calculer le nombre total de pages à récupérer
pages <- ceiling(total_count / limit)
total_count <- fromJSON(json_data)[[1]]
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?group_by=concepts.id")
json_data <- content(response, as = "text")
json_data
total_count <- fromJSON(json_data)[[1]]
View(total_count)
total_count[["count"]]
total_count
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?search=maddi")
json_data <- content(response, as = "text")
total_count <- fromJSON(json_data)[[1]]
View(total_count)
# Définir le nombre d'éléments à récupérer par page
limit <- 200
# Calculer le nombre total de pages à récupérer
pages <- ceiling(total_count$count / limit)
# Initialiser un data frame vide pour stocker les résultats
df_publications <- data.frame()
# Récupérer les données pour chaque page
for (page in 1:pages) {
# Construire l'URL de la requête avec le paramètre de page actuel
url <- paste0("https://api.openalex.org/works?group_by=institutions.country_code&limit=", limit, "&offset=", (page-1)*limit)
# Envoyer la requête
response <- GET(url)
json_data <- content(response, as = "text")
# Ajouter les données de la page actuelle au data frame
df_page <- as.data.frame(fromJSON(json_data))
df_publications <- rbind(df_publications, df_page)
}
# Récupérer les données pour chaque page
for (page in 1:pages) {
# Construire l'URL de la requête avec le paramètre de page actuel
url <- paste0("https://api.openalex.org/works?search=maddi&limit=", limit, "&offset=", (page-1)*limit)
# Envoyer la requête
response <- GET(url)
json_data <- content(response, as = "text")
# Ajouter les données de la page actuelle au data frame
df_page <- as.data.frame(fromJSON(json_data))
df_publications <- rbind(df_publications, df_page)
}
View(df_publications)
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
df_publications <- as.data.frame(json_data$group_by)
View(df_publications)
View(total_count)
View(json_data)
json_data[["meta"]]
## explo rapide
glimpse(df)
=======
library(stringr)
library(tibble)
library(tidytext)
library(textdata)
library(Hmisc)
library(zoo)
library(flextable)
library(DBI)
library(data.table)
library(tidyverse)
library(trimmer)
library(DescTools)
library(questionr)
library(RPostgres)
library(lubridate)
library(timechange)
library(urltools)
library(stringr)
library(rebus)
library(Matrix)
library(plyr)
library(sjmisc)
library(regexplain)
library(gtsummary)
library(igraph)
library(openxlsx2)
library(httr)
library(rvest)
library(XML)
# Connexion ----
con<-dbConnect(RPostgres::Postgres())
db <- 'SKEPTISCIENCE'  #provide the name of your db
host_db <- 'localhost' # server
db_port <- '5433'  # port DBA
db_user <- 'postgres' # nom utilisateur
db_password <- 'Maroua1912'
con <- dbConnect(RPostgres::Postgres(), dbname = db, host=host_db, port=db_port, user=db_user, password=db_password)
# Test connexion
dbListTables(con)
### Récupération des données ----
reqsql= paste('select inner_id, publication, "DateCreated" as date_com, html as comm from data_commentaires')
data_comm = dbGetQuery(con,reqsql)
# Transformer le format de la date du commentaire
data_comm$date_com <- as.Date.character(data_comm$date_com)
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%http%","%www%","%WWW%","%HTTP%"))
# Créer une nouvelle colonne pour stocker les URL extraites
URL_var$urls <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(URL_var)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- str_extract_all(URL_var$comm[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
URL_var$urls[i] <- extracted_url
}
# séparer les URLs en autant de lignes
df_split <- unnest(URL_var, urls)
urls_parses <- url_parse(df_split$urls)
df <- merge(df_split, urls_parses, by = "row.names", all = F)
df$urls_sans_html <- sapply(df$urls2, function(x) gsub("[>/\\<]", "", x))
df$urls_sans_html <- sapply(df$urls, function(x) gsub("[>/\\<]", "", x))
View(df)
df$urls_sans_html <- sapply(df$urls, function(x) gsub("[>/\\<]", " ", x))
##
df$urls3 <- NA
for (i in 1:nrow(df[1:1000,])) {
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))")
# Stocker l'URL extraite dans la nouvelle colonne
df$urls3[i] <- extracted_url
}
View(df)
View(df)
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
View(df)
1:nrow(df[1:1000,])
View(df)
for (i in 1:nrow(df[1:1000])) {
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
df$urls3[i] <- extracted_url
}
for (i in 1:nrow(df[,1:1000])) {
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
df$urls3[i] <- extracted_url
}
for (i in 1:nrow(df[1:1000,])) {
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
df$urls3[i] <- extracted_url
}
View(df)
df$urls_sans_html <- sapply(df$urls, function(x) gsub("[>\\<]", " ", x))
##
df$urls3 <- NA
for (i in 1:nrow(df[1:1000,])) {
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
df$urls3[i] <- extracted_url
}
View(df)
for (i in 1:nrow(df[1:10000,])) {
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
df$urls3[i] <- extracted_url
}
View(df)
for (i in 1:nrow(df)) {
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
df$urls3[i] <- extracted_url
}
View(df)
# Retirer les balises html
URL_var$comm_sans_html <- sapply(URL_var$comm, function(x) gsub("[>\\<]", " ", x))
View(URL_var)
# Créer une nouvelle colonne pour stocker les URL extraites
URL_var$urls <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(URL_var)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- str_extract_all(URL_var$comm_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
URL_var$urls[i] <- extracted_url
}
# séparer les URLs en autant de lignes
df_split <- unnest(URL_var, urls)
urls_parses <- url_parse(df_split$urls)
df <- merge(df_split, urls_parses, by = "row.names", all = F)
View(df)
# Fonction pour supprimer les balises HTML
remove_html_tags <- function(x) {
gsub("<.*?>", "", x) # .*? correspond à n'importe quel caractère répété 0 ou plusieurs fois, de manière non gourmande
}
URL_var$comm_sans_html <- sapply(URL_var$comm, remove_html_tags)
View(URL_var)
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%http%","%www%","%WWW%","%HTTP%"))
# Fonction pour supprimer les balises HTML
remove_html_tags <- function(x) {
gsub("<.*?>", "", x) # .*? correspond à n'importe quel caractère répété 0 ou plusieurs fois, de manière non gourmande
}
# Application de la fonction à la colonne "comm"
URL_var$comm_sans_html <- sapply(URL_var$comm, remove_html_tags)
View(URL_var)
# Créer une nouvelle colonne pour stocker les URL extraites
URL_var$urls <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(URL_var)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- str_extract_all(URL_var$comm_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
URL_var$urls[i] <- extracted_url
}
# séparer les URLs en autant de lignes
df_split <- unnest(URL_var, urls)
urls_parses <- url_parse(df_split$urls)
df <- merge(df_split, urls_parses, by = "row.names", all = F)
View(df)
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%http%","%www%","%WWW%","%HTTP%"))
# Fonction pour supprimer les balises HTML
remove_html_tags <- function(x) {
gsub("<.*?>", " ", x) # .*? correspond à n'importe quel caractère répété 0 ou plusieurs fois, de manière non gourmande
}
# Application de la fonction à la colonne "comm"
URL_var$comm_sans_html <- sapply(URL_var$comm, remove_html_tags)
# Créer une nouvelle colonne pour stocker les URL extraites
URL_var$urls <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(URL_var)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- str_extract_all(URL_var$comm_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
URL_var$urls[i] <- extracted_url
}
# "(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))"
# séparer les URLs en autant de lignes
df_split <- unnest(URL_var, urls)
View(URL_var)
library(xml2)
URL_var$comm_sans_html <- sapply(URL_var$comm_sans_html, function(x) {
html_text(read_html(x))
}
URL_var$comm_sans_html <- sapply(URL_var$comm_sans_html, function(x) {
URL_var$comm_sans_html <- sapply(URL_var$comm_sans_html, function(x) {
html_text(read_html(x))
})
# suppression des balises HTML
URL_var$comm_sans_html <- sapply(URL_var$comm_sans_html, function(x) {
html_text(read_html(x))
})
URL_var$comm_sans_html <- sapply(URL_var$comm_sans_html, function(x) {
doc <- read_html(x)
nodes <- html_nodes(doc, "strong")
xml_remove(nodes)
as.character(doc)
})
URL_var$comm_sans_html <- sapply(URL_var$comm_sans_html, function(x) {
doc <- read_html(x)
nodes <- html_nodes(doc, "strong")
xml_remove(nodes)
as.character(doc)
})
# suppression des balises strong
URL_var$comm <- sapply(URL_var$comm_sans_html, function(x) {
doc <- read_html(x)
nodes <- html_nodes(doc, "strong")
xml_remove(nodes)
as.character(doc)
})
# suppression des balises strong
URL_var$comm <- sapply(URL_var$comm_, function(x) {
doc <- read_html(x)
nodes <- html_nodes(doc, "strong")
xml_remove(nodes)
as.character(doc)
})
# suppression des balises strong
URL_var$comm <- sapply(URL_var$comm, function(x) {
doc <- read_html(x)
nodes <- html_nodes(doc, "strong")
xml_remove(nodes)
as.character(doc)
})
View(URL_var)
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%http%","%www%","%WWW%","%HTTP%"))
rm(list = ls()) #supprimer tous les objets
library(stringr)
library(tibble)
library(tidytext)
library(textdata)
library(Hmisc)
library(zoo)
library(flextable)
library(DBI)
library(data.table)
library(tidyverse)
library(trimmer)
library(DescTools)
library(questionr)
library(RPostgres)
library(lubridate)
library(timechange)
library(urltools)
library(stringr)
library(rebus)
library(Matrix)
library(plyr)
library(sjmisc)
library(regexplain)
library(gtsummary)
library(igraph)
library(openxlsx2)
library(httr)
library(rvest)
library(XML)
# Connexion ----
con<-dbConnect(RPostgres::Postgres())
db <- 'SKEPTISCIENCE'  #provide the name of your db
host_db <- 'localhost' # server
db_port <- '5433'  # port DBA
db_user <- 'postgres' # nom utilisateur
db_password <- 'Maroua1912'
con <- dbConnect(RPostgres::Postgres(), dbname = db, host=host_db, port=db_port, user=db_user, password=db_password)
# Test connexion
dbListTables(con)
### Récupération des données ----
reqsql= paste('select inner_id, publication, "DateCreated" as date_com, html as comm from data_commentaires')
data_comm = dbGetQuery(con,reqsql)
# Transformer le format de la date du commentaire
data_comm$date_com <- as.Date.character(data_comm$date_com)
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%http%","%www%","%WWW%","%HTTP%"))
# suppression des balises strong
URL_var$comm_sans_html <- sapply(URL_var$comm, function(x) {
doc <- read_html(x)
nodes <- html_nodes(doc, "strong")
xml_remove(nodes)
as.character(doc)
})
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%http%","%www%","%WWW%","%HTTP%"))
rm(list = ls()) #supprimer tous les objets
library(stringr)
library(tibble)
library(tidytext)
library(textdata)
library(Hmisc)
library(zoo)
library(flextable)
library(DBI)
library(data.table)
library(tidyverse)
library(trimmer)
library(DescTools)
library(questionr)
library(RPostgres)
library(lubridate)
library(timechange)
library(urltools)
library(stringr)
library(rebus)
library(Matrix)
library(plyr)
library(sjmisc)
library(regexplain)
library(gtsummary)
library(igraph)
library(openxlsx2)
library(httr)
library(rvest)
library(XML)
# Connexion ----
con<-dbConnect(RPostgres::Postgres())
db <- 'SKEPTISCIENCE'  #provide the name of your db
host_db <- 'localhost' # server
db_port <- '5433'  # port DBA
db_user <- 'postgres' # nom utilisateur
db_password <- 'Maroua1912'
con <- dbConnect(RPostgres::Postgres(), dbname = db, host=host_db, port=db_port, user=db_user, password=db_password)
# Test connexion
dbListTables(con)
### Récupération des données ----
reqsql= paste('select inner_id, publication, "DateCreated" as date_com, html as comm from data_commentaires')
data_comm = dbGetQuery(con,reqsql)
# Transformer le format de la date du commentaire
data_comm$date_com <- as.Date.character(data_comm$date_com)
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%http%","%www%","%WWW%","%HTTP%"))
# Fonction pour supprimer les balises HTML
remove_html_tags <- function(x) {
gsub("<.*?>", " ", x) # .*? correspond à n'importe quel caractère répété 0 ou plusieurs fois, de manière non gourmande
}
# Application de la fonction à la colonne "comm"
URL_var$comm_sans_html <- sapply(URL_var$comm, remove_html_tags)
View(URL_var)
# Créer une nouvelle colonne pour stocker les URL extraites
URL_var$urls <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(URL_var)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- str_extract_all(URL_var$comm_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
URL_var$urls[i] <- extracted_url
}
View(URL_var)
# Fonction pour supprimer les balises HTML
remove_html_tags <- function(x) {
gsub("[>\\<]", " ", x) # .*? correspond à n'importe quel caractère répété 0 ou plusieurs fois, de manière non gourmande
}
# Application de la fonction à la colonne "comm"
URL_var$comm_sans_html <- sapply(URL_var$comm, remove_html_tags)
View(URL_var)
# Créer une nouvelle colonne pour stocker les URL extraites
URL_var$urls <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(URL_var)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- str_extract_all(URL_var$comm_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
URL_var$urls[i] <- extracted_url
}
df_split <- unnest(URL_var, urls)
urls_parses <- url_parse(df_split$urls)
df <- merge(df_split, urls_parses, by = "row.names", all = F)
View(df)
View(urls_parses)
View(urls_parses)
View(URL_var)
View(df_split)
View(df)
View(df)
urls_vf <- data.frame(df$Row.names,df$publication,df$inner_id,df$date_com,df[,7:14])
View(urls_vf)
names(urls_vf)[,1:4] <- c("rowname","publication","inner_id","date_com")
names(urls_vf[,1:4]) <- c("rowname","publication","inner_id","date_com")
View(urls_vf)
names(urls_vf[1:4]) <- c("rowname","publication","inner_id","date_com")
urls_vf <- data.frame(df$Row.names,df$publication,df$inner_id,df$date_com,df[,7:14])
names(urls_vf[,1:4]) <- c("rowname","publication","inner_id","date_com")
names(urls_vf)[,1:4] <- c("rowname","publication","inner_id","date_com")
colnames(urls_vf)[,1:4] <- c("rowname","publication","inner_id","date_com")
names(urls_vf)[1:4] <- c("rowname","publication","inner_id","date_com")
View(urls_vf)
urls_v1 <- data.frame(df$Row.names,df$publication,df$inner_id,df$date_com,df[,7:14])
urls_vf <- urls_v1 %>%
unique()
View(urls_vf)
urls_vf <- urls_v1[,-1] %>%
unique()
View(urls_vf)
View(df)
urls_vf[289125]
urls_vf[289125,]
urls_vf$urls[289125,]
urls_vf$urls[289125]
urls_vf$urls[row_number=="289125"]
urls_vf$urls[row_number="289125"]
urls_vf$urls[row.names="289125"]
urls_vf$urls[row.names=289125]
View(urls_vf)
urls_v1[urls_v1$server==""]
a <- urls_v1[urls_v1$server==""]
urls_vf$urls[urls_vf$df.publication=="68"]
a <- urls_v1[urls_v1$server==""]
a <- subset(urls_v1, urls_v1$server==" ")
a <- subset(urls_v1, urls_v1$server=="")
View(a)
# Extraire le protocole (http://)
protocole <- gsub("^(.*://).*", "\\1", a$urls)
# Extraire le nom de domaine (www.example.com)
domaine <- gsub("^.*://([^/]+).*", "\\1", a$urls)
# Extraire le chemin (/path/to/)
chemin <- gsub("^.*://[^/]+(/.*/).*", "\\1", a$urls)
# Extraire le nom de fichier (file.html)
nom_fichier <- gsub("^.*://[^/]+/.*/(.*)$", "\\1", a$urls)
b <- data.frame(protocole,domaine,chemin,chemin)
View(b)
b <- data.frame(protocole,domaine,chemin)
View(b)
View(df)
View(urls_vf)
View(a)
View(b)
View(urls_vf)
View(data_comm)
View(urls_vf)
View(df)
f2 <- factor(urls_vf$server) |>
fct_infreq() |>
questionr::freq()
View(f2)
>>>>>>> c71e55c63cf7d8936fec11d3863d640058fbbafb
