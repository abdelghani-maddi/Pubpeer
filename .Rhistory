class_sites2 <- readxl::read_xlsx("classification sites2.xlsx", col_names = TRUE)
# Vecteur contenant les valeurs à exclure
search_strings <- class_sites2$site
# Extraction des lignes pour lesquelles la colonne "element" contient une partie des chaînes de caractères
new_df <- df[!apply(sapply(search_strings, function(x) grepl(x, t2$sit_harm, fixed = FALSE)), 1, any), ]
View(new_df)
# Extraction des lignes pour lesquelles la colonne "element" contient une partie des chaînes de caractères
new_df <- df[!apply(sapply(search_strings, function(x) grepl(x, t2$site, fixed = TRUE)), 1, any), ]
View(new_df)
View(df)
# Extraction des lignes pour lesquelles la colonne "element" contient une partie des chaînes de caractères
new_df <- df[!apply(sapply(search_strings, function(x) grepl(x, df$element, fixed = TRUE)), 1, any), ]
View(new_df)
# Récupérer les données de sites à exclure
class_sites2 <- readxl::read_xlsx("classification sites2.xlsx", col_names = TRUE)
# Vecteur contenant les valeurs à exclure
search_strings <- class_sites2$site
# Extraction des lignes pour lesquelles la colonne "element" contient une partie des chaînes de caractères
new_df <- df[!apply(sapply(search_strings, function(x) grepl(x, df$element, fixed = TRUE)), 1, any), ]
# Extraction des lignes pour lesquelles la colonne "element" contient une partie des chaînes de caractères
new_df <- df[!apply(sapply(search_strings, function(x) grepl(x, df$element), 1, any), ]
# Extraction des lignes pour lesquelles la colonne "element" contient une partie des chaînes de caractères
new_df <- df[!apply(sapply(search_strings, function(x) grepl(x, df$element)), 1, any), ]
View(new_df)
class_sites2 <- readxl::read_xlsx("classification sites2.xlsx", col_names = TRUE)
# Vecteur contenant les valeurs à exclure
search_strings <- class_sites2$site
# Extraction des lignes pour lesquelles la colonne "element" contient une partie des chaînes de caractères
new_df <- df[!apply(sapply(search_strings, function(x) grepl(x, df$element)), 1, any), ]
View(new_df)
# Récupérer les données de sites à exclure
class_sites2 <- readxl::read_xlsx("classification sites2.xlsx", col_names = TRUE)
# Vecteur contenant les valeurs à exclure
search_strings <- class_sites2$site
# Extraction des lignes pour lesquelles la colonne "element" contient une partie des chaînes de caractères
new_df <- df[!apply(sapply(search_strings, function(x) grepl(x, df$element)), 1, any), ]
View(new_df)
f <- factor(new_df$element) |>
fct_infreq() |>
questionr::freq()
f_a_analyser <- data.frame(rownames(f),f)
# Récupérer les données de sites à exclure
class_sites2 <- readxl::read_xlsx("classification sites2.xlsx", col_names = TRUE)
# Vecteur contenant les valeurs à exclure
search_strings <- class_sites2$site
# Extraction des lignes pour lesquelles la colonne "element" contient une partie des chaînes de caractères
new_df <- df[!apply(sapply(search_strings, function(x) grepl(x, df$element)), 1, any), ]
# Affichage du nouveau dataframe
new_df
# Calculer les fréquences pour avoir une idée de la distribution des sites
f <- factor(new_df$element) |>
fct_infreq() |>
questionr::freq()
f_a_analyser <- data.frame(rownames(f),f)
View(f_a_analyser)
# Importer les données téseaurus pour unifier et mettre en forme les sites (ceux qui sont les plus fréquents)
class_sites <- readxl::read_xlsx("classification sites.xlsx", col_names = TRUE)
t2 <- t %>%
left_join(class_sites, by = c("site")) %>% # un left join avec expressions régulières (contain)
data.frame(factor(.$site), factor(.$type_sit)) %>%
.[,c(1,3,6,7)]
names(t2) <- c("publication", "seq","site", "sit_harm")
# Remplacer les valeurs manquantes dans col1 avec les valeurs correspondantes dans col2
t2$site <- as.character(t2$site)
t2$sit_harm <- as.character(t2$sit_harm)
t2$sit_harm[is.na(t2$sit_harm)] <- t2$site[is.na(t2$sit_harm)]
t2$site <- factor(t2$site)
t2$sit_harm <- factor(t2$sit_harm)
## Recoding t$type
t2$type_sit <- t2$type_sit %>%
fct_explicit_na("Autre")
#
f_autre <- t2$site[t2$type_sit=="Autre"] |>
fct_infreq() |>
questionr::freq()
View(f_autre)
View(t2)
## Recoding t$type
t2$type_sit <- t2$type_sit %>%
fct_explicit_na("Autre")
t2 <- t %>%
left_join(class_sites, by = c("site")) %>% # un left join avec expressions régulières (contain)
data.frame(factor(.$site), factor(.$type_sit)) %>%
.[,c(1,3,6,7)]
names(t2) <- c("publication", "seq","site", "sit_harm")
# Remplacer les valeurs manquantes dans col1 avec les valeurs correspondantes dans col2
t2$site <- as.character(t2$site)
t2$sit_harm <- as.character(t2$sit_harm)
t2$sit_harm[is.na(t2$sit_harm)] <- t2$site[is.na(t2$sit_harm)]
t2$site <- factor(t2$site)
t2$sit_harm <- factor(t2$sit_harm)
dbWriteTable(con, "data_sites_harmo", t2)
View(f_autre)
View(t2)
# Importer les données téseaurus pour unifier et mettre en forme les sites (ceux qui sont les plus fréquents)
class_sites <- readxl::read_xlsx("classification sites.xlsx", col_names = TRUE)
t2 <- t %>%
left_join(class_sites, by = c("site")) %>% # un left join avec expressions régulières (contain)
data.frame(factor(.$site), factor(.$type_sit)) %>%
.[,c(1,3,6,7)]
names(t2) <- c("publication", "seq","site", "sit_harm")
# Remplacer les valeurs manquantes dans col1 avec les valeurs correspondantes dans col2
t2$site <- as.character(t2$site)
t2$sit_harm <- as.character(t2$sit_harm)
t2$sit_harm[is.na(t2$sit_harm)] <- t2$site[is.na(t2$sit_harm)]
t2$site <- factor(t2$site)
t2$sit_harm <- factor(t2$sit_harm)
# Récupérer les données de sites à exclure
class_sites2 <- readxl::read_xlsx("classification sites2.xlsx", col_names = TRUE)
# Vecteur contenant les valeurs à exclure
search_strings <- class_sites2$site
# Extraction des lignes pour lesquelles la colonne "element" contient une partie des chaînes de caractères
new_df <- df[!apply(sapply(search_strings, function(x) grepl(x, t2$sit_harm)), 1, any), ]
View(new_df)
# Extraction des lignes pour lesquelles la colonne "element" contient une partie des chaînes de caractères
new_df <- t2[!apply(sapply(search_strings, function(x) grepl(x, t2$sit_harm)), 1, any), ]
View(new_df)
f <- factor(new_df$element) |>
fct_infreq() |>
questionr::freq()
# Calculer les fréquences pour avoir une idée de la distribution des sites
f <- factor(new_df$sit_harm) |>
fct_infreq() |>
questionr::freq()
View(f)
f_a_analyser <- data.frame(rownames(f),f)
View(f_a_analyser)
View(new_df)
# Récupérer les données de sites à exclure
class_sites2 <- readxl::read_xlsx("classification sites2.xlsx", col_names = TRUE)
# Vecteur contenant les valeurs à exclure
search_strings <- class_sites2$site
# Extraction des lignes pour lesquelles la colonne "element" contient une partie des chaînes de caractères
new_df <- t2[!apply(sapply(search_strings, function(x) grepl(x, t2$sit_harm)), 1, any), ]
# Calculer les fréquences pour avoir une idée de la distribution des sites
f <- factor(new_df$sit_harm) |>
fct_infreq() |>
questionr::freq()
f_a_analyser <- data.frame(rownames(f),f)
View(new_df)
View(df)
View(t2)
data_comm$comm[data_comm$publication==60504]
data_comm$comm[data_comm$publication==60381]
View(con)
# Exemple de vecteur de caractères
text <- data_comm$comm
# Extraction des URL à l'aide de la fonction str_extract_all()
urls_orig <- str_extract_all(text, "(https?://[a-z\\.]+)")
# Conversion de la liste en vecteur unique
urls_orig <- unique(unlist(urls_orig))
# Affichage des URL extraites
urls_orig
# Extraction des URL à l'aide de la fonction str_extract_all()
urls_orig <- str_extract_all(text, "(https?://[a-z\\.]+)")
View(urls_orig)
urls_orig[[1]]
data_comm$comm[data_comm$publication==1]
# Expression régulière pour extraire les URLs
url_pattern <- "(https?|ftp)://[^\\s/$.?#].[^\\s]*"
# Extraction des URLs
urls_orig <- regmatches(text, gregexpr(url_pattern, text))
# Affichage des URLs
unlist(urls_orig)
View(urls_orig)
urls_orig[[1]]
data_comm$comm[data_comm$publication==1]
# Exemple de vecteur de caractères
text <- URL_var
# Expression régulière pour extraire les URLs
url_pattern <- "(https?|ftp)://[^\\s/$.?#].[^\\s]*"
# Extraction des URLs
urls_orig <- regmatches(text, gregexpr(url_pattern, text))
View(urls_orig)
urls_orig[["1"]]
View(URL_extract)
urls_orig[["1"]]
urls_orig[["1"]]
names(urls_orig) <- names(URL_var)
# Transformer en liste de dataframe
list_data_ori <- Map(as.data.frame, urls_orig) %>%
rbindlist(., use.names = F, idcol = names(.))
names(list_data_ori) = c("publication", "site")
df_ori = data.frame(list_data_ori$publication, factor(list_data_ori$site))
names(df_ori) = c("publication", "site")
View(df_ori)
View(df)
View(t2)
# Expression régulière pour extraire les URLs
url_pattern <- "(https?://|www\\.)\\S+"
# Extraction des URLs
urls_orig <- regmatches(text, gregexpr(url_pattern, text))
View(urls_orig)
urls_orig[["1"]]
# Expression régulière pour extraire les URLs
url_pattern <- "(?:(?:https?://|www\\.)\\S+|(?<=href=\")[^\"]+(?=\"))"
# Extraction des URLs
urls_orig <- regmatches(text, gregexpr(url_pattern, text))
# Expression régulière pour extraire les URLs
url_pattern <- "(?:(?:https?://|www\\.)\\S+|(?<=href=\")[^\"]+(?=\"))"
# Extraction des URLs
urls_orig <- regmatches(text, gregexpr(url_pattern, text))
# Expression régulière pour extraire les URLs
url_pattern <- "(?:(?:https?://|www\\.)\\S+|(?<=href=\")[^\"]+(?=\"))"
# Extraction des URLs
urls_orig <- regmatches(text, gregexpr(url_pattern, text))
# Expression régulière pour extraire les URLs
url_pattern <- "<a\\s+[^>]*href=\"(https?://|www\\.)[^\"]*\"[^>]*>[^<]*</a>"
# Extraction des URLs
urls_orig <- regmatches(text, gregexpr(url_pattern, text))
View(urls_orig)
urls_orig[["1"]]
urls <- unlist(regmatches(x, gregexpr("(?i)\\bhttps?://\\S+['\"]?", x, perl=TRUE)))
urls_orig <- unlist(regmatches(x, gregexpr("(?i)\\bhttps?://\\S+['\"]?", x, perl=TRUE)))
urls <- unlist(regmatches(urls_orig, gregexpr("(?i)\\bhttps?://\\S+['\"]?", urls_orig, perl=TRUE)))
# Expression régulière pour extraire les URLs
url_pattern <- "(https?://)?(www\\.)?\\w+\\.\\w+(/[\\w\\-./?%&=]*)?"
urls <- str_extract_all(texte, url_pattern)[[1]]
urls <- str_extract_all(text, url_pattern)[[1]]
urls
urls <- str_extract_all(text, "(?<=href=\")[^\"]*")
urls
View(urls)
urls[[1]]
urls <- unlist(regmatches(text, gregexpr("https?://\\S+|www\\.\\S+", gsub("\"", "'", text), perl=TRUE)))
urls[1]
urls
rm(list = ls()) #supprimer tous les objets
# chargement des packages ----
library(stringr)
library(tibble)
library(tidytext)
library(textdata)
library(Hmisc)
library(zoo)
library(flextable)
library(DBI)
library(data.table)
library(tidyverse)
library(trimmer)
library(DescTools)
library(questionr)
library(RPostgres)
library(lubridate)
library(timechange)
library(urltools)
library(stringr)
library(rebus)
library(Matrix)
library(plyr)
library(sjmisc)
library(regexplain)
library(gtsummary)
library(igraph)
# install.packages('remotes')
# remotes::install_github("gadenbuie/regexplain")
# Dossier travail
setwd('/Users/maddi/Documents/Pubpeer project/Pubpeer')
# Connexion ----
con<-dbConnect(RPostgres::Postgres())
db <- 'SKEPTISCIENCE'  #provide the name of your db
host_db <- 'localhost' # server
db_port <- '5433'  # port DBA
db_user <- 'postgres' # nom utilisateur
db_password <- 'Maroua1912'
con <- dbConnect(RPostgres::Postgres(), dbname = db, host=host_db, port=db_port, user=db_user, password=db_password)
# Test connexion
dbListTables(con)
### Récupération des données ----
reqsql= paste('select inner_id, publication, html as comm from data_commentaires')
data_comm = dbGetQuery(con,reqsql)
### Récupération des commentaires avec liens hypertextes ----
# Creer la liste des liens qui se trouvent dans les commentaires gardant
# le lien avec les publications à partir desquelles sont extraits
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%https%","%http%","%www%","%WWW%")) %>% # Etape 1
#### Etape 2 : Recupérer uniquement les commentaires (l'intérêt de garder le subset est de pouvoir selectionner par la suite les disciplines aussi) ----
.$comm # Etape 2
#### Etape 3 : nommer le vecteur avec l'identifiant des publications ----
names(URL_var) <- subset(data_comm$publication, data_comm$comm %like% c("%https%","%http%","%www%","%WWW%")) # Etape 3
#### Etape 4 : Créer un pattern pour l'extraction des URL ----
pat<- "//" %R% capture(one_or_more(char_class(WRD,DOT)))
#### Etape 5 : Utiliser "rebus" pour extraire l'URL principal ----
URL_extract<-str_extract_all(URL_var, "(?<=//)[^\\s/:]+") #URL_extract<-str_match_all(URL_var, pattern = pat)
# Attribuer aux liens, les identifiants des publications d'où ils sont issus ----
names(URL_extract) <- names(URL_var)
# Transformer en liste de dataframe
list_data <- Map(as.data.frame, URL_extract) %>%
rbindlist(., use.names = F, idcol = names(.))
names(list_data) = c("publication", "site")
df = data.frame(list_data$publication, factor(list_data$site))
names(df) = c("publication", "site")
# list_data[list_data$publication == "106541"] -- petit test -- OK :)
# Nettoyer les données et préparer des fichiers txt pour vosviewer ----
`%not_like%` <- purrr::negate(`%like%`) # juste au cas où j'aurais besoin du not_like.
`%not_in%` <- purrr::negate(`%in%`) # juste au cas où j'aurais besoin du not_like.
donnees <- list_data[df$site %like% "%\\.%"]
# Le site doit contenir au moins un caractère alphabetique
donnees1 <- donnees[grep("[a-zA-Z]", donnees$site), ]
# Supprimer tous les caractères spéciaux sauf le "."
donnees2 <- data.frame(donnees1$publication, stringr::str_remove_all(donnees1$site, "[\\p{P}\\p{S}&&[^.]]"))
names(donnees2) = c("publication", "site")
# Transformation des données (sites as integer)
donnees3 <- data.frame(as.integer(donnees2$publication), donnees2$site)
names(donnees3) = c("publication", "site")
## Exportation des données ----
#  dbWriteTable(conn = con, "publication_sites_comm", donnees3)
# Typologie des sites ----
## Transformer en "factor" les sites
t <- data.frame(donnees3$publication, factor(donnees3$site))
names(t) <- c("publication","site")
# Utiliser la fonction ave() pour ajouter une colonne avec une séquence numérique qui se réinitialise selon id
t$seq <- ave(t$publication, t$publication, FUN = function(x) seq_along(x))
# Utiliser la fonction group_by() pour regrouper les données par identifiant
grouped_df <- t %>%
group_by(publication) %>%
mutate(quartile = ntile(seq, 4))
# Calculer les fréquences pour avoir une idée de la distribution des sites
f <- grouped_df$site |>
fct_infreq() |>
questionr::freq()
# juste pour rajouter la noms de lignes en tant que colonne
freqsit <- data.frame(rownames(f),f)
names(freqsit) = c("site","nb","part","freq")
# Importer les données téseaurus pour unifier et mettre en forme les sites (ceux qui sont les plus fréquents)
class_sites <- readxl::read_xlsx("classification sites.xlsx", col_names = TRUE)
t2 <- t %>%
left_join(class_sites, by = c("site")) %>% # un left join avec expressions régulières (contain)
data.frame(factor(.$site), factor(.$type_sit)) %>%
.[,c(1,3,6,7)]
names(t2) <- c("publication", "seq","site", "sit_harm")
# Remplacer les valeurs manquantes dans col1 avec les valeurs correspondantes dans col2
t2$site <- as.character(t2$site)
t2$sit_harm <- as.character(t2$sit_harm)
t2$sit_harm[is.na(t2$sit_harm)] <- t2$site[is.na(t2$sit_harm)]
t2$site <- factor(t2$site)
t2$sit_harm <- factor(t2$sit_harm)
dbWriteTable(con, "data_sites_harmo", t2)
# Typologie des sites ----
## Transformer en "factor" les sites
t <- data.frame(donnees3$publication, factor(donnees3$site))
names(t) <- c("publication","site")
# Utiliser la fonction ave() pour ajouter une colonne avec une séquence numérique qui se réinitialise selon id
t$seq <- ave(t$publication, t$publication, FUN = function(x) seq_along(x))
# Utiliser la fonction group_by() pour regrouper les données par identifiant
grouped_df <- t %>%
group_by(publication) %>%
mutate(quartile = ntile(seq, 4))
# Calculer les fréquences pour avoir une idée de la distribution des sites
f <- grouped_df$site |>
fct_infreq() |>
questionr::freq()
# juste pour rajouter la noms de lignes en tant que colonne
freqsit <- data.frame(rownames(f),f)
names(freqsit) = c("site","nb","part","freq")
# Importer les données téseaurus pour unifier et mettre en forme les sites (ceux qui sont les plus fréquents)
class_sites <- readxl::read_xlsx("classification sites.xlsx", col_names = TRUE)
t2 <- t %>%
left_join(class_sites, by = c("site")) %>% # un left join avec expressions régulières (contain)
data.frame(factor(.$site), factor(.$type_sit)) %>%
.[,c(1,3,6,7)]
names(t2) <- c("publication", "seq","site", "sit_harm")
# Remplacer les valeurs manquantes dans col1 avec les valeurs correspondantes dans col2
t2$site <- as.character(t2$site)
t2$sit_harm <- as.character(t2$sit_harm)
t2$sit_harm[is.na(t2$sit_harm)] <- t2$site[is.na(t2$sit_harm)]
t2$site <- factor(t2$site)
t2$sit_harm <- factor(t2$sit_harm)
#dbWriteTable(con, "data_sites_harmo", t2)
# exclure du dataframe toutes les lignes (sites) dont on sait qu'ils ne sont pas des sites de discussion
# Récupérer les données de sites à exclure
class_sites2 <- readxl::read_xlsx("classification sites2.xlsx", col_names = TRUE)
# Vecteur contenant les valeurs à exclure
search_strings <- class_sites2$site
# Extraction des lignes pour lesquelles la colonne "element" contient une partie des chaînes de caractères
new_df <- t2[!apply(sapply(search_strings, function(x) grepl(x, t2$sit_harm)), 1, any), ]
# Calculer les fréquences pour avoir une idée de la distribution des sites
f <- factor(new_df$sit_harm) |>
fct_infreq() |>
questionr::freq()
f_a_analyser <- data.frame(rownames(f),f)
View(f_a_analyser)
View(data_comm)
View(df)
data_comm$comm[data_comm$publication==1]
View(df)
View(t2)
View(f_a_analyser)
data_comm$comm[data_comm$publication==66853]
View(f_a_analyser)
data_comm$comm[data_comm$publication==1]
View(f_a_analyser)
View(grouped_df)
ids <- seq_along(groups)
# Créer un dataframe à partir de la liste groups et des identifiants
df <- data.frame(id = rep(ids, lengths(groups)),
element = unlist(groups))
# Créer un vecteur d'identifiants pour chaque groupe
ids <- seq_along(groups)
# Afficher les groupes
groups
View(f_a_analyser)
View(f_a_analyser)
View(f_a_analyser)
View(t2)
# Importer les données téseaurus pour unifier et mettre en forme les sites (ceux qui sont les plus fréquents)
class_sites <- readxl::read_xlsx("classification sites.xlsx", col_names = TRUE)
t2 <- t %>%
left_join(class_sites, by = c("site")) %>% # un left join avec expressions régulières (contain)
data.frame(factor(.$site), factor(.$type_sit)) %>%
.[,c(1,3,6,7)]
names(t2) <- c("publication", "seq","site", "sit_harm")
# Remplacer les valeurs manquantes dans col1 avec les valeurs correspondantes dans col2
t2$site <- as.character(t2$site)
t2$sit_harm <- as.character(t2$sit_harm)
t2$sit_harm[is.na(t2$sit_harm)] <- t2$site[is.na(t2$sit_harm)]
t2$site <- factor(t2$site)
t2$sit_harm <- factor(t2$sit_harm)
f2 <- factor(t2$sit_harm) |>
fct_infreq() |>
questionr::freq()
View(f2)
f2 <- factor(t2$sit_harm) |>
fct_infreq() |>
questionr::freq()
f_a_analyser <- data.frame(rownames(f2),f2)
View(f_a_analyser)
f_a_analyser <- data.frame(rownames(f2),f2$n,f$`val%`)
f_a_analyser <- data.frame(rownames(f2),f2$n,f2$`val%`)
names(f_a_analyser) <- c("site", "nbr_apparitions", "part")
View(f_a_analyser)
View(f_a_analyser)
View(f_a_analyser)
library(xlsx)
install.packages("xslx")
library(xlsx)
java -version
install.packages("rJava")
library(xlsx)
library(rJava)
install.packages("xlsx")
library(xlsx)
library(rJava)
library(rJava)
.jinit()
install.packages('rJava')
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
if (Sys.getenv("JAVA_HOME")!="")
Sys.setenv(JAVA_HOME="")
library(rJava)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
library(rJava)
# Créer une nouvelle colonne pour stocker les URL extraites
data_comm$urls <- NA
nrow(data_comm)
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(data_comm)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(data_comm$comm[i], regexpr("(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", data_comm$comm))
# Stocker l'URL extraite dans la nouvelle colonne
data_comm$urls[i] <- extracted_url
}
library(stringr)
library(dplyr)
# Créer une nouvelle colonne pour stocker les URL extraites
data_comm$urls <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(data_comm)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(data_comm$comm[i], regexpr("(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", data_comm$comm))
# Stocker l'URL extraite dans la nouvelle colonne
data_comm$urls[i] <- extracted_url
}
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(data_comm$comm[i], regexpr("(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", data_comm$comm))
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(data_comm$comm[i], regexpr("(?i)\\b((?:https?://|www\\d{{0,3}}[.]|[a-z0-9.\\-]+[.][a-z]{{2,4}}/)(?:[^\\s()<>]+|(\\([^\\s()<>]+\\)))*+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", data_comm$comm))
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(data_comm)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(data_comm$comm[i], regexpr("(?i)\\b((?:https?://|www\\d{{0,3}}[.]|[a-z0-9.\\-]+[.][a-z]{{2,4}}/)(?:[^\\s()<>]+|(\\([^\\s()<>]+\\)))*+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", data_comm$comm))
# Stocker l'URL extraite dans la nouvelle colonne
data_comm$urls[i] <- extracted_url
}
# Créer un vecteur de test contenant des URL
urls <- c("https://www.example.com/1", "https://www.example.com/2", "https://www.example.com/3")
# Créer une dataframe contenant une colonne de test avec les URL
df <- data.frame(colonne_texte = c("Voici une URL : https://www.example.com/1", "Une autre URL : https://www.example.com/2", "Encore une URL : https://www.example.com/3"))
# Créer une nouvelle colonne pour stocker les URL extraites
df$new_colonne <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(df)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(df$colonne_texte[i], regexpr("(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", df$colonne_texte))
# Stocker l'URL extraite dans la nouvelle colonne
df$new_colonne[i] <- extracted_url
}
# Créer un vecteur de test contenant des URL
urls <- c("https://www.example.com/1", "https://www.example.com/2", "https://www.example.com/3")
# Créer une dataframe contenant une colonne de test avec les URL
df <- data.frame(colonne_texte = c("Voici une URL : https://www.example.com/1", "Une autre URL : https://www.example.com/2", "Encore une URL : https://www.example.com/3"))
# Créer une nouvelle colonne pour stocker les URL extraites
df$new_colonne <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(df)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(df$colonne_texte[i], regexpr("(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", df$colonne_texte))
# Stocker l'URL extraite dans la nouvelle colonne
df$new_colonne[i] <- extracted_url
}
