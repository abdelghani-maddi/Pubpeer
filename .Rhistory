}
rm(list = ls()) #supprimer tous les objets
library(tidyverse)
library(questionr)
library(gtsummary)
### Récupération des données ----
## importer les données quand elles sont en local et non sur Pstgresql
df <- readxl::read_excel("D:/bdd/data_pub.xlsx")
# revues
jnal <- select(df, starts_with("Journal"))
library(httr)
library(jsonlite)
library(jsonlite)
# Envoyer la requête à l'API openalex
response <- GET("https://api.openalex.org/publications?year=2021&groupby=country")
# Convertir la réponse en JSON
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
# Convertir les données JSON en data frame
df <- as.data.frame(json_data)
View(response)
View(df)
# Envoyer la requête à l'API openalex
response <- GET("https://api.openalex.org/publications?year=2021&groupby=country")
# Convertir la réponse en JSON
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
# Envoyer la requête à l'API openalex
response <- GET("https://api.openalex.org/institutions?group_by=country_code")
# Convertir la réponse en JSON
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
# Convertir les données JSON en data frame
df <- as.data.frame(json_data)
View(response)
response[["status_code"]]
response <- GET("https://api.openalex.org/institutions?group_by=country_code")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
df_institutions <- as.data.frame(json_data)
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/institutions?group_by=country_code")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
df_institutions <- as.data.frame(json_data)
View(json_data)
json_data[["group_by"]]
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/works?group_by=country_code")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
df_institutions <- as.data.frame(json_data)
View(json_data)
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
json_data <- content(response, as = "text")
json_data <- content(response, as = "text")
View(response)
json_data <- content(response, as = "text")
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
json_data <- content(response, as = "text")
View(response)
response[["status_code"]]
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
View(response)
json_data <- content(response, as = "text")
library(tidyverse)
library(questionr)
library(gtsummary)
library(httr)
library(jsonlite)
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
View(response)
response[["status_code"]]
# Obtenir une liste des institutions de recherche par pays
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
json_data <- content(response, as = "text")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
df_publications <- as.data.frame(json_data)
View(json_data)
json_data[["group_by"]]
df_publications <- as.data.frame(json_data)
df_publications <- as.data.frame(json_data$group_by)
View(df_publications)
# Fonction pour récupérer le nombre de publications pour une institution donnée
get_publication_count <- function(institution_id, year) {
url <- paste0("https://api.openalex.org/institutions/", institution_id, "/publications")
response <- GET(url, query = list(year = year))
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
return(json_data$count)
}
# Initialiser un data frame pour stocker les volumes de publication par pays
df_publications <- data.frame(country_code = character(),
publication_count = integer())
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
df_publications <- as.data.frame(json_data$group_by)
View(df_publications)
tbl_summary(df_publications)
View(df_publications)
nb_openalex <- df_publications |>
fct_infreq() |>
questionr::freq()
nb_openalex <- df_publications$count |>
fct_infreq() |>
questionr::freq()
View(df_publications)
## importer les données quand elles sont en local et non sur Pstgresql
df <- readxl::read_excel("D:/bdd/data_pub.xlsx")
## explo rapide
glimpse(df)
summary(df$concepts_mot)
describe(df$concepts_mot)
describe(df$Journal_Categories_WOS)
describe(df$Journal_Domaines_WOS)
describe(df$Journal_Domaines_Scimago)
describe(df$concepts_mot)
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?filter=concepts_count:%3E2")
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?filter=concepts_count:%3E2")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
df_concepts <- as.data.frame(json_data$group_by)
View(json_data)
json_data[["results"]]
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?filter=concepts_count:%3E2")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
View(json_data)
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?filter=concepts_count:%3E0")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
View(json_data)
json_data[["results"]]
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?group_by=concepts.id")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
View(json_data)
json_data[["group_by"]]
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
View(json_data)
df_publications <- as.data.frame(json_data$group_by)
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?group_by=concepts.id")
json_data <- content(response, as = "text")
total_count <- fromJSON(json_data)[[1]]
View(total_count)
# Définir le nombre d'éléments à récupérer par page
limit <- 200
# Calculer le nombre total de pages à récupérer
pages <- ceiling(total_count / limit)
total_count <- fromJSON(json_data)[[1]]
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?group_by=concepts.id")
json_data <- content(response, as = "text")
json_data
total_count <- fromJSON(json_data)[[1]]
View(total_count)
total_count[["count"]]
total_count
# Obtenir le nombre de publications par pays dans la base openalex
response <- GET("https://api.openalex.org/works?search=maddi")
json_data <- content(response, as = "text")
total_count <- fromJSON(json_data)[[1]]
View(total_count)
# Définir le nombre d'éléments à récupérer par page
limit <- 200
# Calculer le nombre total de pages à récupérer
pages <- ceiling(total_count$count / limit)
# Initialiser un data frame vide pour stocker les résultats
df_publications <- data.frame()
# Récupérer les données pour chaque page
for (page in 1:pages) {
# Construire l'URL de la requête avec le paramètre de page actuel
url <- paste0("https://api.openalex.org/works?group_by=institutions.country_code&limit=", limit, "&offset=", (page-1)*limit)
# Envoyer la requête
response <- GET(url)
json_data <- content(response, as = "text")
# Ajouter les données de la page actuelle au data frame
df_page <- as.data.frame(fromJSON(json_data))
df_publications <- rbind(df_publications, df_page)
}
# Récupérer les données pour chaque page
for (page in 1:pages) {
# Construire l'URL de la requête avec le paramètre de page actuel
url <- paste0("https://api.openalex.org/works?search=maddi&limit=", limit, "&offset=", (page-1)*limit)
# Envoyer la requête
response <- GET(url)
json_data <- content(response, as = "text")
# Ajouter les données de la page actuelle au data frame
df_page <- as.data.frame(fromJSON(json_data))
df_publications <- rbind(df_publications, df_page)
}
View(df_publications)
response <- GET("https://api.openalex.org/works?group_by=institutions.country_code")
json_data <- content(response, as = "text")
json_data <- fromJSON(json_data)
df_publications <- as.data.frame(json_data$group_by)
View(df_publications)
View(total_count)
View(json_data)
json_data[["meta"]]
## explo rapide
glimpse(df)
rm(list = ls()) #supprimer tous les objets
library(tidyverse)
library(questionr)
library(RPostgres)
library(lubridate)
library(urltools)
library(TraMineR)
library(cluster)
library(seqhandbook)
library(ade4)
library(explor)
library(FactoMineR)
library(factoextra)
library(labelled)
library(readxl)
donnees_urls <- read_excel("D:/bdd/donnees_urls.xlsx")
## Préparation des données ----
data_urls$sequence <- as.numeric(data_urls$sequence)
rm(list = ls()) #supprimer tous les objets
# en local :
data_urls <- readxl::read_excel("D:/bdd/data_urls.xlsx")
## Préparation des données ----
data_urls$sequence <- as.numeric(data_urls$sequence)
data_urls$publication <- as.numeric(data_urls$publication)
# Fonction pour calculer la position de chaque valeur
calculer_position <- function(x) {
position <- seq_along(x)
position / length(x)
}
# Regrouper les données par identifiant de publication
data_urls_grouped <- data_urls %>% group_by(publication)
# Ajouter une nouvelle colonne avec la position de chaque élément dans la séquence
data_urls_position <- data_urls_grouped %>% mutate(position = calculer_position(sequence))
# Ajouter une colonne avec la valeur maximale de "sequence" pour chaque "id" : cela correspond au nombre de liens par publication
data_max_sequence <- data_urls_position %>%
group_by(publication) %>%
mutate(max_sequence = max(sequence))
## Recoding data_max_sequence$annee into data_max_sequence$annee_rec
data_max_sequence$annee_rec <- data_max_sequence$annee %>%
as.character() %>%
fct_recode(
"2013-15" = "2013",
"2013-15" = "2014",
"2013-15" = "2015",
"2016-18" = "2016",
"2016-18" = "2017",
"2016-18" = "2018",
"2019-21" = "2019",
"2019-21" = "2020",
"2019-21" = "2021"
)
# utiliser la fonction aggregate pour calculer la moyenne, grou by domain et annee
df <- aggregate(position ~ typo + annee_rec, data = subset(data_max_sequence, max_sequence>1), mean) # se limiter aux publications avec au moins 2 liens
# Transformer les moyennes en quartiles
df_quartiles <- df %>%
mutate(quartile = ntile(position, 4))
# Pivoter l'annee pour n'analyse des séquences
df_pivot <- df_quartiles[,c(1,2,4)] %>%
pivot_wider(names_from = annee_rec, values_from = quartile, values_fill = 0)
# Convertir les données en format de séquence
sequences <- as.matrix(df_pivot[,2:4])
# Supprimer les valeurs manquantes
sequences[is.na(sequences)] <- "-"
# définir les lables pour les différents états
labels <- c("Q1", "Q2", "Q3", "Q4")
seq <- seqdef(sequences, states = labels)
couts <- seqsubm(seq, method = "CONSTANT", cval = 2)
seq.om <- seqdist(seq, method = "OM", indel = 1, sm = couts)
seq.dist <- hclust(as.dist(seq.om), method = "ward.D2")
plot(as.dendrogram(seq.dist), leaflab = "none")
plot(sort(seq.dist$height, decreasing = TRUE)[1:13], type = "s", xlab = "nb de classes", ylab = "inertie")
nbcl <- 4
seq.part <- cutree(seq.dist, nbcl)
seq.part <- factor(seq.part, labels = paste("classe", 1:nbcl, sep = "."))
seqdplot(seq, group = seq.part, xtlab = c("2013-2015", "2016-2018","2019-2021"), border = NA)
seqIplot(seq, group = seq.part, xtlab = c("2013-2015", "2016-2018","2019-2021"), space = 0, border = NA, yaxis = FALSE)
seq_heatmap(seq, seq.dist, labCol = c("2013-2015", "2016-2018","2019-2021"), cexCol = 0.9)
seqfplot(seq, group = seq.part)
seqmsplot(seq, group = seq.part, xtlab = c("2013-2015", "2016-2018","2019-2021"), main = "")
seqmtplot(seq, group = seq.part)
seqrplot(seq, group = seq.part, dist.matrix = seq.om, criterion = "dist")
seqHtplot(seq, group = seq.part, xtlab = c("2013-2015", "2016-2018","2019-2021"))
## ACP
df_pivot <- mutate_if(df_pivot, is.integer, as.numeric)
row.names(df_pivot) <- df_pivot$typo
t <- df_pivot[,2:4] %>%
as.data.frame()
row.names(t) <- df_pivot$typo
str(t)
res.pca <- PCA(t)
explor::explor(res.pca)
ind <- get_pca_ind(res.pca)
# Coordonnées des individus
head(ind$coord)
# Qualité des individus
head(ind$cos2)
# Contributions des individus
head(ind$contrib)
fviz_pca_ind (res.pca, col.ind = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Évite le chevauchement de texte
)
fviz_pca_ind (res.pca, pointsize = "cos2",
pointshape = 21, fill = "#E7B800",
repel = TRUE # Évite le chevauchement de texte
)
res.hcpc <- HCPC(res.pca, graph = FALSE)
plot(res.hcpc, choice = "3D.map")
View(t)
seq <- seqdef(sequences, states = labels)
View(seq)
acm <- dudi.acm(seq)
View(seq)
row.names(seq) <- df_pivot$typo
View(seq)
acm <- dudi.acm(seq)
View(seq)
acm <- dudi.acm(factor(seq))
factor(seq)
df_acm <- seq
View(df_acm)
View(df_acm)
df_acm <- as.data.frame(seq)
View(df)
View(df_acm)
class(df_acm)
row.names(df_acm) <- df_pivot$typo
View(df_acm)
acm <- dudi.acm(df_acm)
explor::explor(res.pca)
View(res.pca)
View(t)
explor::explor(res.pca)
View(data_max_sequence)
df <- aggregate(position ~ typo + annee_rec, data = subset(data_max_sequence, max_sequence>3), mean) # se limiter aux publications avec au moins 2 liens
# Transformer les moyennes en quartiles
df_quartiles <- df %>%
mutate(quartile = ntile(position, 4))
# Pivoter l'annee pour n'analyse des séquences
df_pivot <- df_quartiles[,c(1,2,4)] %>%
pivot_wider(names_from = annee_rec, values_from = quartile, values_fill = 0)
###
# Convertir les données en format de séquence
sequences <- as.matrix(df_pivot[,2:4])
# Supprimer les valeurs manquantes
sequences[is.na(sequences)] <- "-"
# définir les lables pour les différents états
labels <- c("Q1", "Q2", "Q3", "Q4")
seq <- seqdef(sequences, states = labels)
couts <- seqsubm(seq, method = "CONSTANT", cval = 2)
seq.om <- seqdist(seq, method = "OM", indel = 1, sm = couts)
seq.dist <- hclust(as.dist(seq.om), method = "ward.D2")
plot(as.dendrogram(seq.dist), leaflab = "none")
plot(sort(seq.dist$height, decreasing = TRUE)[1:13], type = "s", xlab = "nb de classes", ylab = "inertie")
nbcl <- 4
seq.part <- cutree(seq.dist, nbcl)
seq.part <- factor(seq.part, labels = paste("classe", 1:nbcl, sep = "."))
seqdplot(seq, group = seq.part, xtlab = c("2013-2015", "2016-2018","2019-2021"), border = NA)
seqIplot(seq, group = seq.part, xtlab = c("2013-2015", "2016-2018","2019-2021"), space = 0, border = NA, yaxis = FALSE)
seq_heatmap(seq, seq.dist, labCol = c("2013-2015", "2016-2018","2019-2021"), cexCol = 0.9)
# utiliser la fonction aggregate pour calculer la moyenne, grou by domain et annee
df <- aggregate(position ~ typo + annee_rec, data = subset(data_max_sequence, max_sequence>9), mean) # se limiter aux publications avec au moins 2 liens
# Transformer les moyennes en quartiles
df_quartiles <- df %>%
mutate(quartile = ntile(position, 4))
# Pivoter l'annee pour n'analyse des séquences
df_pivot <- df_quartiles[,c(1,2,4)] %>%
pivot_wider(names_from = annee_rec, values_from = quartile, values_fill = 0)
###
# Convertir les données en format de séquence
sequences <- as.matrix(df_pivot[,2:4])
# Supprimer les valeurs manquantes
sequences[is.na(sequences)] <- "-"
# définir les lables pour les différents états
labels <- c("Q1", "Q2", "Q3", "Q4")
seq <- seqdef(sequences, states = labels)
couts <- seqsubm(seq, method = "CONSTANT", cval = 2)
seq.om <- seqdist(seq, method = "OM", indel = 1, sm = couts)
seq.dist <- hclust(as.dist(seq.om), method = "ward.D2")
plot(as.dendrogram(seq.dist), leaflab = "none")
plot(sort(seq.dist$height, decreasing = TRUE)[1:13], type = "s", xlab = "nb de classes", ylab = "inertie")
nbcl <- 4
nbcl <- 6
seq.part <- cutree(seq.dist, nbcl)
seq.part <- factor(seq.part, labels = paste("classe", 1:nbcl, sep = "."))
seqdplot(seq, group = seq.part, xtlab = c("2013-2015", "2016-2018","2019-2021"), border = NA)
nb_urls_pub <- data_max_sequence %>%
select(publication, max_sequence) %>%
unique()
View(nb_urls_pub)
esquisse:::esquisser()
summary(nb_urls_pub)
summary(nb_urls_pub$max_sequence)
nb_urls_pub <- data_max_sequence %>%
select(publication, max_sequence) %>%
subset(., max_sequence>1)
nb_urls_pub <- data_max_sequence %>%
select(publication, max_sequence) %>%
subset(., max_sequence>1) %>%
unique()
summary(nb_urls_pub$max_sequence)
library(gtsummary)
tbl_summary(nb_urls_pub)
tbl_summary(nb_urls_pub$max_sequence)
tbl_summary(nb_urls_pub)
theme_gtsummary_language(language = "fr", decimal.mark = ",", big.mark = " ")
tbl_summary(nb_urls_pub,
include = c(max_sequence))
# gtsummary
par(mfrow=c(1,2))
theme_gtsummary_language(language = "fr", decimal.mark = ",", big.mark = " ")
tbl_summary(nb_urls_pub,
include = c(max_sequence))
theme_gtsummary_mean_sd()
tbl_summary(nb_urls_pub,
include = c(max_sequence))
par(mfrow=c(1,1))
tbl_summary(nb_urls_pub,
include = c(max_sequence))
# utiliser la fonction aggregate pour calculer la moyenne, grou by domain et annee
df <- aggregate(position ~ typo + annee_rec, data = subset(data_max_sequence, max_sequence>1), mean) # se limiter aux publications avec au moins 2 liens
data = subset(data_max_sequence, max_sequence>1)
View(data_max_sequence)
library(questionr)
# describe
describe(data_max_sequence$max_sequence)
# describe
f <- factor(data_max_sequence$max_sequence) |>
fct_infreq() |>
questionr::freq()
freqsit <- data.frame(rownames(f),f)
View(freqsit)
names(freqsit) <- c("Nombre de liens dans les commentaires", "Nombre de publications", "Part")
write_xlsx(freqsit, "D:/Analyse/Stats/distrib nb liens pub.xlsx")
library(openxlsx)
write_xlsx(freqsit, "D:/Analyse/Stats/distrib nb liens pub.xlsx")
write.xlsx(freqsit, "D:/Analyse/Stats/distrib nb liens pub.xlsx")
# describe
f <- factor(nb_urls_pub$max_sequence) |>
fct_infreq() |>
questionr::freq()
library(tidyverse)
library(questionr)
library(RPostgres)
library(lubridate)
library(urltools)
library(TraMineR)
library(cluster)
library(seqhandbook)
library(ade4)
library(explor)
library(FactoMineR)
library(factoextra)
library(labelled)
library(gtsummary)
library(questionr)
f <- factor(nb_urls_pub$max_sequence) |>
fct_infreq() |>
questionr::freq()
View(f)
freqsit <- data.frame(rownames(f),f)
names(freqsit) <- c("Nombre de liens dans les commentaires", "Nombre de publications", "Part")
write.xlsx(freqsit, "D:/Analyse/Stats/distrib nb liens pub.xlsx")
View(freqsit)
nb_urls_pub <- data_max_sequence %>%
select(publication, max_sequence) %>%
#subset(., max_sequence>1) %>%
unique()
f <- factor(nb_urls_pub$max_sequence) |>
fct_infreq() |>
questionr::freq()
freqsit <- data.frame(rownames(f),f)
names(freqsit) <- c("Nombre de liens dans les commentaires", "Nombre de publications", "Part")
write.xlsx(freqsit, "D:/Analyse/Stats/distrib nb liens pub.xlsx")
# utiliser la fonction aggregate pour calculer la moyenne, grou by domain et annee
df <- aggregate(position ~ typo + annee_rec, data = subset(data_max_sequence, max_sequence>1), mean) # se limiter aux publications avec au moins 2 liens
View(data_urls)
# Analyse de la distribution du nombre de liens par publication
nb_comm_pub <- data_max_sequence %>%
select(publication, inner_id) %>%
#subset(., max_sequence>1) %>%
unique()
View(nb_comm_pub)
View(data_urls)
View(t)
explor::explor(res.pca)
explor::explor(res.pca)
View(t)
View(data_max_sequence)
# Calcul de la fréquence des sites pour avoir une idée plus précise
f <- factor(urls_unique$domain[urls_unique$typo == "Médias"]) |>
fct_infreq() |>
questionr::freq()
