nrow(data_comm)
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(data_comm)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(data_comm$comm[i], regexpr("(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", data_comm$comm))
# Stocker l'URL extraite dans la nouvelle colonne
data_comm$urls[i] <- extracted_url
}
library(stringr)
library(dplyr)
# Créer une nouvelle colonne pour stocker les URL extraites
data_comm$urls <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(data_comm)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(data_comm$comm[i], regexpr("(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", data_comm$comm))
# Stocker l'URL extraite dans la nouvelle colonne
data_comm$urls[i] <- extracted_url
}
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(data_comm$comm[i], regexpr("(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", data_comm$comm))
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(data_comm$comm[i], regexpr("(?i)\\b((?:https?://|www\\d{{0,3}}[.]|[a-z0-9.\\-]+[.][a-z]{{2,4}}/)(?:[^\\s()<>]+|(\\([^\\s()<>]+\\)))*+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", data_comm$comm))
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(data_comm)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(data_comm$comm[i], regexpr("(?i)\\b((?:https?://|www\\d{{0,3}}[.]|[a-z0-9.\\-]+[.][a-z]{{2,4}}/)(?:[^\\s()<>]+|(\\([^\\s()<>]+\\)))*+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", data_comm$comm))
# Stocker l'URL extraite dans la nouvelle colonne
data_comm$urls[i] <- extracted_url
}
# Créer un vecteur de test contenant des URL
urls <- c("https://www.example.com/1", "https://www.example.com/2", "https://www.example.com/3")
# Créer une dataframe contenant une colonne de test avec les URL
df <- data.frame(colonne_texte = c("Voici une URL : https://www.example.com/1", "Une autre URL : https://www.example.com/2", "Encore une URL : https://www.example.com/3"))
# Créer une nouvelle colonne pour stocker les URL extraites
df$new_colonne <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(df)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(df$colonne_texte[i], regexpr("(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", df$colonne_texte))
# Stocker l'URL extraite dans la nouvelle colonne
df$new_colonne[i] <- extracted_url
}
# Créer un vecteur de test contenant des URL
urls <- c("https://www.example.com/1", "https://www.example.com/2", "https://www.example.com/3")
# Créer une dataframe contenant une colonne de test avec les URL
df <- data.frame(colonne_texte = c("Voici une URL : https://www.example.com/1", "Une autre URL : https://www.example.com/2", "Encore une URL : https://www.example.com/3"))
# Créer une nouvelle colonne pour stocker les URL extraites
df$new_colonne <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(df)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- regmatches(df$colonne_texte[i], regexpr("(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))", df$colonne_texte))
# Stocker l'URL extraite dans la nouvelle colonne
df$new_colonne[i] <- extracted_url
}
rm(list = ls()) #supprimer tous les objets
library(stringr)
library(tibble)
library(tidytext)
library(textdata)
library(Hmisc)
library(zoo)
library(flextable)
library(DBI)
library(data.table)
library(tidyverse)
library(trimmer)
library(DescTools)
library(questionr)
library(RPostgres)
library(lubridate)
library(timechange)
library(urltools)
library(stringr)
library(rebus)
library(Matrix)
library(plyr)
library(sjmisc)
library(regexplain)
library(gtsummary)
library(igraph)
library(openxlsx2)
library(httr)
library(rvest)
library(XML)
# Connexion ----
con<-dbConnect(RPostgres::Postgres())
db <- 'SKEPTISCIENCE'  #provide the name of your db
host_db <- 'localhost' # server
db_port <- '5433'  # port DBA
db_user <- 'postgres' # nom utilisateur
db_password <- 'Maroua1912'
con <- dbConnect(RPostgres::Postgres(), dbname = db, host=host_db, port=db_port, user=db_user, password=db_password)
# Test connexion
dbListTables(con)
### Récupération des données ----
reqsql= paste('select inner_id, publication, "DateCreated" as date_com, html as comm from data_commentaires')
data_comm = dbGetQuery(con,reqsql)
# Transformer le format de la date du commentaire
data_comm$date_com <- as.Date.character(data_comm$date_com)
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%http%","%www%","%WWW%","%HTTP%"))
# Créer une nouvelle colonne pour stocker les URL extraites
URL_var$urls <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(URL_var)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- str_extract_all(URL_var$comm[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
URL_var$urls[i] <- extracted_url
}
# séparer les URLs en autant de lignes
df_split <- unnest(URL_var, urls)
urls_parses <- url_parse(df_split$urls)
df <- merge(df_split, urls_parses, by = "row.names", all = F)
df$urls_sans_html <- sapply(df$urls2, function(x) gsub("[>/\\<]", "", x))
df$urls_sans_html <- sapply(df$urls, function(x) gsub("[>/\\<]", "", x))
View(df)
df$urls_sans_html <- sapply(df$urls, function(x) gsub("[>/\\<]", " ", x))
##
df$urls3 <- NA
for (i in 1:nrow(df[1:1000,])) {
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))")
# Stocker l'URL extraite dans la nouvelle colonne
df$urls3[i] <- extracted_url
}
View(df)
View(df)
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
View(df)
1:nrow(df[1:1000,])
View(df)
for (i in 1:nrow(df[1:1000])) {
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
df$urls3[i] <- extracted_url
}
for (i in 1:nrow(df[,1:1000])) {
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
df$urls3[i] <- extracted_url
}
for (i in 1:nrow(df[1:1000,])) {
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
df$urls3[i] <- extracted_url
}
View(df)
df$urls_sans_html <- sapply(df$urls, function(x) gsub("[>\\<]", " ", x))
##
df$urls3 <- NA
for (i in 1:nrow(df[1:1000,])) {
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
df$urls3[i] <- extracted_url
}
View(df)
for (i in 1:nrow(df[1:10000,])) {
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
df$urls3[i] <- extracted_url
}
View(df)
for (i in 1:nrow(df)) {
# Extraire l'URL en utilisant une expression régulière : c'est pour les cas où la chaine de caractères contient un balisage HTML avec des caractères spéciaux qui ne sont pas pris en compte par l'expression régulière précédente
extracted_url <- str_extract_all(df$urls_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
df$urls3[i] <- extracted_url
}
View(df)
# Retirer les balises html
URL_var$comm_sans_html <- sapply(URL_var$comm, function(x) gsub("[>\\<]", " ", x))
View(URL_var)
# Créer une nouvelle colonne pour stocker les URL extraites
URL_var$urls <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(URL_var)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- str_extract_all(URL_var$comm_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
URL_var$urls[i] <- extracted_url
}
# séparer les URLs en autant de lignes
df_split <- unnest(URL_var, urls)
urls_parses <- url_parse(df_split$urls)
df <- merge(df_split, urls_parses, by = "row.names", all = F)
View(df)
# Fonction pour supprimer les balises HTML
remove_html_tags <- function(x) {
gsub("<.*?>", "", x) # .*? correspond à n'importe quel caractère répété 0 ou plusieurs fois, de manière non gourmande
}
URL_var$comm_sans_html <- sapply(URL_var$comm, remove_html_tags)
View(URL_var)
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%http%","%www%","%WWW%","%HTTP%"))
# Fonction pour supprimer les balises HTML
remove_html_tags <- function(x) {
gsub("<.*?>", "", x) # .*? correspond à n'importe quel caractère répété 0 ou plusieurs fois, de manière non gourmande
}
# Application de la fonction à la colonne "comm"
URL_var$comm_sans_html <- sapply(URL_var$comm, remove_html_tags)
View(URL_var)
# Créer une nouvelle colonne pour stocker les URL extraites
URL_var$urls <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(URL_var)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- str_extract_all(URL_var$comm_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
URL_var$urls[i] <- extracted_url
}
# séparer les URLs en autant de lignes
df_split <- unnest(URL_var, urls)
urls_parses <- url_parse(df_split$urls)
df <- merge(df_split, urls_parses, by = "row.names", all = F)
View(df)
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%http%","%www%","%WWW%","%HTTP%"))
# Fonction pour supprimer les balises HTML
remove_html_tags <- function(x) {
gsub("<.*?>", " ", x) # .*? correspond à n'importe quel caractère répété 0 ou plusieurs fois, de manière non gourmande
}
# Application de la fonction à la colonne "comm"
URL_var$comm_sans_html <- sapply(URL_var$comm, remove_html_tags)
# Créer une nouvelle colonne pour stocker les URL extraites
URL_var$urls <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(URL_var)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- str_extract_all(URL_var$comm_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
URL_var$urls[i] <- extracted_url
}
# "(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))"
# séparer les URLs en autant de lignes
df_split <- unnest(URL_var, urls)
View(URL_var)
library(xml2)
URL_var$comm_sans_html <- sapply(URL_var$comm_sans_html, function(x) {
html_text(read_html(x))
}
URL_var$comm_sans_html <- sapply(URL_var$comm_sans_html, function(x) {
URL_var$comm_sans_html <- sapply(URL_var$comm_sans_html, function(x) {
html_text(read_html(x))
})
# suppression des balises HTML
URL_var$comm_sans_html <- sapply(URL_var$comm_sans_html, function(x) {
html_text(read_html(x))
})
URL_var$comm_sans_html <- sapply(URL_var$comm_sans_html, function(x) {
doc <- read_html(x)
nodes <- html_nodes(doc, "strong")
xml_remove(nodes)
as.character(doc)
})
URL_var$comm_sans_html <- sapply(URL_var$comm_sans_html, function(x) {
doc <- read_html(x)
nodes <- html_nodes(doc, "strong")
xml_remove(nodes)
as.character(doc)
})
# suppression des balises strong
URL_var$comm <- sapply(URL_var$comm_sans_html, function(x) {
doc <- read_html(x)
nodes <- html_nodes(doc, "strong")
xml_remove(nodes)
as.character(doc)
})
# suppression des balises strong
URL_var$comm <- sapply(URL_var$comm_, function(x) {
doc <- read_html(x)
nodes <- html_nodes(doc, "strong")
xml_remove(nodes)
as.character(doc)
})
# suppression des balises strong
URL_var$comm <- sapply(URL_var$comm, function(x) {
doc <- read_html(x)
nodes <- html_nodes(doc, "strong")
xml_remove(nodes)
as.character(doc)
})
View(URL_var)
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%http%","%www%","%WWW%","%HTTP%"))
rm(list = ls()) #supprimer tous les objets
library(stringr)
library(tibble)
library(tidytext)
library(textdata)
library(Hmisc)
library(zoo)
library(flextable)
library(DBI)
library(data.table)
library(tidyverse)
library(trimmer)
library(DescTools)
library(questionr)
library(RPostgres)
library(lubridate)
library(timechange)
library(urltools)
library(stringr)
library(rebus)
library(Matrix)
library(plyr)
library(sjmisc)
library(regexplain)
library(gtsummary)
library(igraph)
library(openxlsx2)
library(httr)
library(rvest)
library(XML)
# Connexion ----
con<-dbConnect(RPostgres::Postgres())
db <- 'SKEPTISCIENCE'  #provide the name of your db
host_db <- 'localhost' # server
db_port <- '5433'  # port DBA
db_user <- 'postgres' # nom utilisateur
db_password <- 'Maroua1912'
con <- dbConnect(RPostgres::Postgres(), dbname = db, host=host_db, port=db_port, user=db_user, password=db_password)
# Test connexion
dbListTables(con)
### Récupération des données ----
reqsql= paste('select inner_id, publication, "DateCreated" as date_com, html as comm from data_commentaires')
data_comm = dbGetQuery(con,reqsql)
# Transformer le format de la date du commentaire
data_comm$date_com <- as.Date.character(data_comm$date_com)
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%http%","%www%","%WWW%","%HTTP%"))
# suppression des balises strong
URL_var$comm_sans_html <- sapply(URL_var$comm, function(x) {
doc <- read_html(x)
nodes <- html_nodes(doc, "strong")
xml_remove(nodes)
as.character(doc)
})
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%http%","%www%","%WWW%","%HTTP%"))
rm(list = ls()) #supprimer tous les objets
library(stringr)
library(tibble)
library(tidytext)
library(textdata)
library(Hmisc)
library(zoo)
library(flextable)
library(DBI)
library(data.table)
library(tidyverse)
library(trimmer)
library(DescTools)
library(questionr)
library(RPostgres)
library(lubridate)
library(timechange)
library(urltools)
library(stringr)
library(rebus)
library(Matrix)
library(plyr)
library(sjmisc)
library(regexplain)
library(gtsummary)
library(igraph)
library(openxlsx2)
library(httr)
library(rvest)
library(XML)
# Connexion ----
con<-dbConnect(RPostgres::Postgres())
db <- 'SKEPTISCIENCE'  #provide the name of your db
host_db <- 'localhost' # server
db_port <- '5433'  # port DBA
db_user <- 'postgres' # nom utilisateur
db_password <- 'Maroua1912'
con <- dbConnect(RPostgres::Postgres(), dbname = db, host=host_db, port=db_port, user=db_user, password=db_password)
# Test connexion
dbListTables(con)
### Récupération des données ----
reqsql= paste('select inner_id, publication, "DateCreated" as date_com, html as comm from data_commentaires')
data_comm = dbGetQuery(con,reqsql)
# Transformer le format de la date du commentaire
data_comm$date_com <- as.Date.character(data_comm$date_com)
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%http%","%www%","%WWW%","%HTTP%"))
# Fonction pour supprimer les balises HTML
remove_html_tags <- function(x) {
gsub("<.*?>", " ", x) # .*? correspond à n'importe quel caractère répété 0 ou plusieurs fois, de manière non gourmande
}
# Application de la fonction à la colonne "comm"
URL_var$comm_sans_html <- sapply(URL_var$comm, remove_html_tags)
View(URL_var)
# Créer une nouvelle colonne pour stocker les URL extraites
URL_var$urls <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(URL_var)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- str_extract_all(URL_var$comm_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
URL_var$urls[i] <- extracted_url
}
View(URL_var)
# Fonction pour supprimer les balises HTML
remove_html_tags <- function(x) {
gsub("[>\\<]", " ", x) # .*? correspond à n'importe quel caractère répété 0 ou plusieurs fois, de manière non gourmande
}
# Application de la fonction à la colonne "comm"
URL_var$comm_sans_html <- sapply(URL_var$comm, remove_html_tags)
View(URL_var)
# Créer une nouvelle colonne pour stocker les URL extraites
URL_var$urls <- NA
# Parcourir chaque ligne et extraire l'URL
for (i in 1:nrow(URL_var)) {
# Extraire l'URL en utilisant une expression régulière
extracted_url <- str_extract_all(URL_var$comm_sans_html[i], "(?i)\\b(?:https?://|www\\.)\\S+(?:/|\\b)")
# Stocker l'URL extraite dans la nouvelle colonne
URL_var$urls[i] <- extracted_url
}
df_split <- unnest(URL_var, urls)
urls_parses <- url_parse(df_split$urls)
df <- merge(df_split, urls_parses, by = "row.names", all = F)
View(df)
View(urls_parses)
View(urls_parses)
View(URL_var)
View(df_split)
View(df)
View(df)
urls_vf <- data.frame(df$Row.names,df$publication,df$inner_id,df$date_com,df[,7:14])
View(urls_vf)
names(urls_vf)[,1:4] <- c("rowname","publication","inner_id","date_com")
names(urls_vf[,1:4]) <- c("rowname","publication","inner_id","date_com")
View(urls_vf)
names(urls_vf[1:4]) <- c("rowname","publication","inner_id","date_com")
urls_vf <- data.frame(df$Row.names,df$publication,df$inner_id,df$date_com,df[,7:14])
names(urls_vf[,1:4]) <- c("rowname","publication","inner_id","date_com")
names(urls_vf)[,1:4] <- c("rowname","publication","inner_id","date_com")
colnames(urls_vf)[,1:4] <- c("rowname","publication","inner_id","date_com")
names(urls_vf)[1:4] <- c("rowname","publication","inner_id","date_com")
View(urls_vf)
urls_v1 <- data.frame(df$Row.names,df$publication,df$inner_id,df$date_com,df[,7:14])
urls_vf <- urls_v1 %>%
unique()
View(urls_vf)
urls_vf <- urls_v1[,-1] %>%
unique()
View(urls_vf)
View(df)
urls_vf[289125]
urls_vf[289125,]
urls_vf$urls[289125,]
urls_vf$urls[289125]
urls_vf$urls[row_number=="289125"]
urls_vf$urls[row_number="289125"]
urls_vf$urls[row.names="289125"]
urls_vf$urls[row.names=289125]
View(urls_vf)
urls_v1[urls_v1$server==""]
a <- urls_v1[urls_v1$server==""]
urls_vf$urls[urls_vf$df.publication=="68"]
a <- urls_v1[urls_v1$server==""]
a <- subset(urls_v1, urls_v1$server==" ")
a <- subset(urls_v1, urls_v1$server=="")
View(a)
# Extraire le protocole (http://)
protocole <- gsub("^(.*://).*", "\\1", a$urls)
# Extraire le nom de domaine (www.example.com)
domaine <- gsub("^.*://([^/]+).*", "\\1", a$urls)
# Extraire le chemin (/path/to/)
chemin <- gsub("^.*://[^/]+(/.*/).*", "\\1", a$urls)
# Extraire le nom de fichier (file.html)
nom_fichier <- gsub("^.*://[^/]+/.*/(.*)$", "\\1", a$urls)
b <- data.frame(protocole,domaine,chemin,chemin)
View(b)
b <- data.frame(protocole,domaine,chemin)
View(b)
View(df)
View(urls_vf)
View(a)
View(b)
View(urls_vf)
View(data_comm)
View(urls_vf)
View(df)
f2 <- factor(urls_vf$server) |>
fct_infreq() |>
questionr::freq()
View(f2)
