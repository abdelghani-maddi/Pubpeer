exclure <- c(")",
"^![](http:",
"![file](https:",
")![file](![https:",
")![file](https:",
").",
"),",
"	)![](http:",
"2F",
")![](http:",
")2F,",
")https:",
")](http:",
"http://)",
"http://![](http://))![](http://)",
"http://),",
"http://)![](http://)![](http://)",
"http://)![](http://)",
"http://).",
"http://)2F,"
)
## liens bizares :
liens_bizarres_pubpeer <- c("63537.dots[[1L]][[33043L]]","76578.dots[[1L]][[26350L]]",
"113418.dots[[1L]][[39689L]]2","113418.dots[[1L]][[39689L]]1",
"113420.dots[[1L]][[31132L]]1","118405.dots[[1L]][[32151L]]2",
"117826.dots[[1L]][[21884L]]","115998.dots[[1L]][[36866L]]",
"114885.dots[[1L]][[41539L]]1","67612.dots[[1L]][[43811L]]",
"109088.dots[[1L]][[43462L]]","114001.dots[[1L]][[19235L]]2",
"111799.dots[[1L]][[30785L]]","63909.dots[[1L]][[37628L]]1",
"88844.dots[[1L]][[35579L]]","70109.dots[[1L]][[33647L]]",
"83198.dots[[1L]][[35048L]]3","67963.dots[[1L]][[37872L]]1",
"67963.dots[[1L]][[33470L]]1","80644.dots[[1L]][[38623L]]1",
"80216.dots[[1L]][[27218L]]2","74268.dots[[1L]][[33960L]]",
"72915.dots[[1L]][[40567L]]","72295.dots[[1L]][[4591L]]",
"69018.dots[[1L]][[46509L]]","68341.dots[[1L]][[37910L]]1",
"67014.dots[[1L]][[24815L]]","66618.dots[[1L]][[2156L]]",
"64133.dots[[1L]][[37672L]]2","2575.dots[[1L]][[32547L]]",
"62970.dots[[1L]][[24101L]]1","62780.dots[[1L]][[353L]]2",
"62618.dots[[1L]][[40206L]]","69030.dots[[1L]][[25197L]]1",
"67347.dots[[1L]][[2576L]]","64991.dots[[1L]][[1743L]]1",
"115031.dots[[1L]][[39771L]]1","113418.dots[[1L]][[41484L]]",
"113420.dots[[1L]][[31132L]]2")
lien_bizarre_wiley <- c("63052.dots[[1L]][[24117L]]1")
lien_bizarre_nih <- c("54057.dots[[1L]][[32614L]]1")
lien_bizarre_revue <- ("65248.dots[[1L]][[1914L]]")
## exclusion des liens erronnés et des liens bizarres
# liens erronés
df <- df %>%
subset(., .$liens %not_in% exclure)
#correction liens bizarres
df$liens[rownames(df) %in% liens_bizarres_pubpeer] <- "https://pubpeer.com/"
df$liens[rownames(df) %in% lien_bizarre_nih] <- "https://www.ncbi.nlm.nih.gov/"
df$liens[rownames(df) %in% lien_bizarre_revue] <- "https://mja.com.au/"
df$liens[rownames(df) %in% lien_bizarre_wiley] <- "https://onlinelibrary.wiley.com/"
# supression vecteurs intermédiaires
rm(liens_bizarres_pubpeer)
rm(lien_bizarre_nih)
rm(lien_bizarre_revue)
rm(lien_bizarre_wiley)
## Extraction des sites
parsed_url <- data.frame(df$publication, url_parse(df$liens))
names(parsed_url)[1] = c("publication")
View(pr_id_pub)
url_encode(pr_id_pub$markdown)
encode = url_encode(pr_id_pub$markdown)
encode = data.frame(url_encode(pr_id_pub$markdown))
View(pr_id_pub)
View(encode)
parsed_url$port[parsed_url$publication == "106541"]
parsed_url$port[parsed_url$publication == "106541"]
url_pattern <- "^(?!(.*.com/$)|(.*[es]_[a-z].*)|(.*[a-z]_[es].*).)"
#################################################################################
# Creer la liste des liens qui se trouvent dans les commentaires gardant
# le lien avec les publications à partir desquelles sont extraits
#################################################################################
# transformer le type de données pour plus de facilité/performance dans le traitement
liens_all <- as_tibble(data_comm) %>%
# Se limiter aux commentaires avec au moins un lien hypertexte
subset(., markdown %like% c("%https%","%http%","%www%","%WWW%")) %>%
# Recupérer uniquement les commentaires (l'intérêt de garder le subset est de pouvoir selectionner par la suite les disciplines aussi)
.$markdown %>%
# Extraire les liens
str_extract_all(., url_pattern) %>%
# Transformer les données
Map(as.data.frame, .)
## Formatage espace de travail
rm(list = ls()) #supprimer tous les objets
# activate packages
library(stringr)
library(tibble)
library(tidytext)
library(textdata)
library(Hmisc)
library(zoo)
library(flextable)
library(DBI)
library(data.table)
library(tidyverse)
library(trimmer)
library(DescTools)
library(questionr)
library(RPostgres)
library(lubridate)
library(timechange)
library(urltools)
## Formatage espace de travail ----
rm(list = ls()) #supprimer tous les objets
# chargement des packages ----
library(stringr)
library(tibble)
library(tidytext)
library(textdata)
library(Hmisc)
library(zoo)
library(flextable)
library(DBI)
library(data.table)
library(tidyverse)
library(trimmer)
library(DescTools)
library(questionr)
library(RPostgres)
library(lubridate)
library(timechange)
library(urltools)
library(stringr)
library(rebus)
library(Matrix)
library(plyr)
# Connexion ----
con<-dbConnect(RPostgres::Postgres())
db <- 'SKEPTISCIENCE'  #provide the name of your db
host_db <- 'localhost' # server
db_port <- '5433'  # port DBA
db_user <- 'postgres' # nom utilisateur
db_password <- 'Maroua1912'
con <- dbConnect(RPostgres::Postgres(), dbname = db, host=host_db, port=db_port, user=db_user, password=db_password)
# Test connexion
dbListTables(con)
### Récupération des données ----
reqsql= paste('select inner_id, publication, html as comm from data_commentaires')
data_comm = dbGetQuery(con,reqsql)
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%https%","%http%","%www%","%WWW%")) %>% # Etape 1
#### Etape 2 : Recupérer uniquement les commentaires (l'intérêt de garder le subset est de pouvoir selectionner par la suite les disciplines aussi) ----
.$comm # Etape 2
#### Etape 3 : nommer le vecteur avec l'identifiant des publications ----
names(URL_var) <- subset(data_comm$publication, data_comm$comm %like% c("%https%","%http%","%www%","%WWW%")) # Etape 3
#### Etape 4 : Créer un pattern pour l'extraction des URL ----
pat<- "//" %R% capture(one_or_more(char_class(WRD,DOT)))
#### Etape 5 : Utiliser "rebus" pour extraire l'URL principal ----
URL_extract<-str_extract_all(URL_var, "(?<=//)[^\\s/:]+") #URL_extract<-str_match_all(URL_var, pattern = pat)
# Attribuer aux liens, les identifiants des publications d'où ils sont issus ----
names(URL_extract) <- names(URL_var)
# Attribuer aux liens, les identifiants des publications d'où ils sont issus ----
names(URL_extract) <- names(URL_var)
# Transformer en liste de dataframe
list_data <- Map(as.data.frame, URL_extract) %>%
transform(reshape2::melt(list_data), id = names(list_data))
transform(reshape2::melt(list_data[1:10]), id = names(list_data[1:10]))
# Transformer en liste de dataframe
list_data <- Map(as.data.frame, URL_extract)
transform(reshape2::melt(list_data[1:10]), id = names(list_data[1:10]))
View(list_data)
append(list_data[1:10])
(list_data, list_data[1:10])
append(list_data, list_data[1:10])
append(list_data, list_data[1:10], after = names(list_data))
t <- append(list_data, list_data[1:10])
View(t)
t <- ldply(list_data, list_data[1:10])
t <- ldply(list_data[1:10])
View(t)
t <- ldply(list_data[1000:10000])
t <- ldply(list_data[1000:1050])
View(t)
tt <- merge(t[,1],t[,2])
View(tt)
t <- ldply(list_data[1:10])
tt <- merge(t[,1],t[,2])
View(tt)
t <- ldply(list_data[1:2])
tt <- merge(t[,1],t[,2])
View(t)
t[,1]
tt <- merge(t[,2],t[,3])
View(tt)
View(t)
tt <- rbind(t[,2],t[,3])
View(tt)
tt <- cbind(t[,2],t[,3])
View(tt)
t[,2]
t[,3]
cbind(t[,2],t[,3])
tt <- unite(t[,2],t[,3])
tt <- unite(t, colconc, t[,2],t[,3])
tt <- unite(t, t[,2], t[,2],t[,3], sep = "")
tt <- unite(t, t[,2], t$`dots[[1L]][[1L]]`,t$`dots[[1L]][[2L]]`, sep = "")
tt <- merge(t[,2], t[,3])
View(tt)
tt <- merge(t$.id, t[,2], t[,3])
tt <- merge(t$.id, t[,2], t[,3], by = row.names())
tt <- subset(t, t$`dots[[1L]][[1L]]` != NA)
View(tt)
tt <- subset(t,(!is.na(t[,2])))
View(tt)
tt <- subset(t,(!is.na(t[,1])))
View(t)
tt <- paste(t$`dots[[1L]][[1L]]`, t$`dots[[1L]][[2L]]`)
tt <- data.frame(paste(t$`dots[[1L]][[1L]]`, t$`dots[[1L]][[2L]]`))
View(tt)
tt <- data.frame(.id,paste(t$`dots[[1L]][[1L]]`[t$`dots[[1L]][[1L]]` != is.na()], t$`dots[[1L]][[2L]]`))
tt <- data.frame(.id,paste(t$`dots[[1L]][[1L]]`[t$`dots[[1L]][[1L]]` != is.na(t$`dots[[1L]][[1L]]`)], t$`dots[[1L]][[2L]]`))
tt <- data.frame(t$.id,paste(t$`dots[[1L]][[1L]]`, t$`dots[[1L]][[2L]]`))
View(tt)
tt <- data.frame(t$.id,paste(t$`dots[[1L]][[1L]]`, t$`dots[[1L]][[2L]]`)) %>%
gsub("NA","",.)
View(URL_extract)
tt <- data.frame(t$.id,paste(t$`dots[[1L]][[1L]]`, t$`dots[[1L]][[2L]]`)) %>%
gsub("NA","",.) %>%
data.frame()
View(tt)
tt <- data.frame(t$.id,paste(t$`dots[[1L]][[1L]]`, t$`dots[[1L]][[2L]]`)) %>%
gsub("NA","",.) %>%
as.data.frame()
View(tt)
data.frame(t$.id,paste(t$`dots[[1L]][[1L]]`, t$`dots[[1L]][[2L]]`))
tt <- data.frame(t$.id,paste(t[,1], t[,2])) %>%
gsub("NA","",.) %>%
as.data.frame()
tt <- data.frame(t$.id,paste(t[,1], t[,2]))
View(tt)
tt <- data.frame(t$.id, paste(t[,2], t[,3]))
View(tt)
View(t)
t
t[,2]
t[,2 = is.na()]
t[,is.na(2)]
View(tt)
t <- ldply(list_data[1:3])
tt <- data.frame(t$.id, paste(t[,2], t[,3]))
View(tt)
t <- ldply(list_data[1:4])
tt <- data.frame(t$.id, paste(t[,2], t[,3]))
View(tt)
View(tt)
View(t)
t <- ddply(list_data[1:4])
t <- ldply(list_data[1:4], rbind)
View(t)
fun <- function(x) {
name <- deparse(substitute(x))
x$id <- name
return(x)
}
df.all <- ldply(list_data[1:4], fun)
View(df.all)
list_data[1:4]
df.all <- ldply(list_data[1:4], rbind)
View(df.all)
foo <- function(n, .list){
.list[[n]]$id <- n
.list[[n]]
}
foo(4, list_data[1:4])
a <- foo(4, list_data[1:4])
View(a)
View(df.all)
a <- foo(1:4, list_data[1:4])
a <- foo(1, list_data[1:4])
View(a)
a <- foo(2, list_data[1:4])
View(a)
View(df.all)
.list[[n]]$.id <- n
.list[[n]]$id <- n
foo <- function(n, .list){
.list[[n]]$id <- n
.list[[n]]
}
foo <- function(n, .list){
.list[[n]]$.id <- n
.list[[n]]
}
a <- foo(2, list_data[1:4])
View(a)
View(list_data)
l <- list_data[1:5]
View(l)
df.all <- ldply(l, rbind)
View(df.all)
df.all <- ldply(df.l, rbind)
dd <- lapply(seq_along(l), function(x) cbind(df_name = paste0('df',x),l[[x]]))
do.call(rbind,dd)
View(dd)
df.all <- rbindlist(l)
View(df.all)
df.all <- rbindlist(l, use.names = T)
str(l)
names(l)
df.all <- rbindlist(l, use.names = F)
View(df.all)
df.all <- rbindlist(l, use.names = F, idcol = names(l))
View(df.all)
l <- list_data[1:10]
df.all <- rbindlist(l, use.names = F, idcol = names(l))
View(df.all)
l <- list_data[1:100]
df.all <- rbindlist(l, use.names = F, idcol = names(l))
View(df.all)
# Transformer en liste de dataframe
list_data <- Map(as.data.frame, URL_extract) %>%
rbindlist(., use.names = F, idcol = names(.))
View(list_data)
names(list_data) = c("publication", "site")
list_data[list_data$publication == "106541"]
list_data[site %like% "%twit%"]
list_data[site %like% "%pubpee%"]
View(list_data)
factor(list_data$site)
df = data.frame(publication, factor(list_data$site))
df = data.frame(list_data$publication, factor(list_data$site))
questionr:::irec()
names(df) = c("publication", "site")
questionr:::irec()
View(df)
tst <- urltools::url_parse(df$site)
View(tst)
subset(df, df$site %like% "%.%")
tst <- subset(df, df$site %like% "%.%")
View(tst)
tst <- subset(df, df$site %like% "%//.%")
tst <- subset(df, df$site %like% "%/.%")
tst <- URL_extract<-str_extract_all(df$site, "(?<=//)[^\\s/:]+")
View(tst)
tst <- str_match(df$site, "(?<=//)[^\\s/:]+")
str_match_all(df$site, pattern = "%.%")
df[df$site %like% "%.%"]
list_data[df$site %like% "%pubpee%"]
tst <- list_data[df$site %like% "%pubpee%"]
View(tst)
`%not_like%` <- purrr::negate(`%not_like%`)
`%not_like%` <- purrr::negate(`%like%`)
tst <- list_data[df$site %not_like% "%pubpee%"]
View(tst)
tst2 <- list_data[df$site %like% "%pubpee%"]
View(tst2)
View(tst2)
View(tst)
tst <- list_data[df$site %not_like% "%.%"]
tst2 <- list_data[df$site %like% "%.%"]
View(tst2)
tst <- list_data[df$site %not_like% "."]
View(tst)
str_contains(df, ".", ignore.case = T)
library(sjmisc)
install.packages("sjmisc")
library(sjmisc)
str_contains(df, ".", ignore.case = T)
str_contains(df, "%.%", ignore.case = T)
View(df)
View(df)
str_contains(df, "%\.%", ignore.case = T)
str_contains(df, "%\\.%", ignore.case = T)
tst <- list_data[df$site %not_like% "\\."]
tst <- list_data[df$site %not_like% "%\\.%"]
View(tst)
df %>% list_data[df$site %like% "%\\.%"]
df %>% df[df$site %like% "%\\.%"]
df[df$site %like% "%\\.%"]
tst <- list_data[df$site %like% "%\\.%"]
df[df$site %like% "%\\.%"]
donnees <- list_data[df$site %like% "%\\.%"]
View(donnees)
## Exportation des données ----
dbWriteTable(conn = con, "publication_sites_comm", donnees)
donnees <- list_data[df$site %like% "%\\.%"] %>%
gsub("<", "", .)
donnees <- list_data[df$site %like% "%\\.%"] %>%
gsub("<", "", .) %>%
as.data.frame()
donnees <- list_data[df$site %like% "%\\.%"] %>%
gsub("<", "", .) %>%
data.frame()
donnees <- list_data[df$site %like% "%\\.%"]
donnees2 <- data.frame(donnees$publication, gsub('//<//>//"', "", donnees$site))
View(donnees2)
donnees2 <- data.frame(donnees$publication, gsub('//<//>//"//?', "", donnees$site))
View(donnees2)
View(donnees2)
donnees2 <- data.frame(donnees$publication, gsub('//<//>//"//?//(//)', "", donnees$site))
## Exportation des données ----
dbWriteTable(conn = con, "publication_sites_comm", donnees2)
## Exportation des données ----
dbWriteTable(conn = con, "publication_sites_comm", donnees2)
names(donnees2) = c("publication", "site")
## Exportation des données ----
dbWriteTable(conn = con, "publication_sites_comm", donnees2)
donnees2 <- data.frame(donnees$publication, gsub(pattern = '//<//>//"//?//(//)', "", donnees$site))
names(donnees2) = c("publication", "site")
donnees2$site[donnees2$site %like% '%//"%']
donnees2$site[donnees2$site %like% '%//<%']
donnees2 <- data.frame(donnees$publication, gsub(regex('//<//>//"//?//(//)'), "", donnees$site))
names(donnees2) = c("publication", "site")
View(donnees2)
donnees2 <- data.frame(donnees$publication, gsub("[:punct:]", "", donnees$site))
names(donnees2) = c("publication", "site")
View(donnees2)
gsub("[:punct:]", "", donnees$site)
ttt <- data.frame(gsub("[:punct:]", "", donnees$site))
View(ttt)
ttt <- data.frame(gsub("[:punct:]", "-----", donnees$site))
View(ttt)
ttt <- data.frame(gsub("[:punct:]", " ", donnees$site))
View(donnees2)
ttt <- data.frame(gsub( pattern = "[:punct:]", " ", donnees$site))
View(ttt)
ttt <- data.frame(gsub( pattern = "[:punct:]", ".", donnees$site))
View(ttt)
ttt <- data.frame(gsub( pattern = '\\?\\<\\>\\"', "", donnees$site))
View(ttt)
ttt <- data.frame(gsub( pattern = '^\\?\\<\\>\\"', "", donnees$site))
View(ttt)
ttt <- data.frame(gsub(pattern = '^\\?\\<\\>\\"', "-", donnees$site))
View(ttt)
ttt <- data.frame(gsub(pattern = '//?\\<\\>\\"', "-", donnees$site))
View(tt)
ttt <- data.frame(gsub(pattern = '//?\\<\\>\\"', "-", donnees$site))
View(ttt)
ttt <- data.frame(gsub("[[:punct:]]", "", donnees, except(".", "-")))
install.packages(c("BH", "broom", "class", "cli", "colorspace", "curl", "dbplyr", "dplyr", "evaluate", "expm", "fansi", "flextable", "fontawesome", "forcats", "fs", "httpuv", "knitr", "lubridate", "MASS", "odbc", "officer", "pbkrtest", "pryr", "purrr", "Rcpp", "RcppTOML", "reticulate", "rmarkdown", "RPostgres", "sass", "spatial", "stringi", "styler", "survival", "tidyr", "timechange", "vctrs", "vroom", "xgboost", "yaml"))
install.packages(c("BH", "broom", "class", "cli", "colorspace", "curl", "dbplyr", "dplyr", "evaluate", "expm", "fansi", "flextable", "fontawesome", "forcats", "fs", "httpuv", "knitr", "lubridate", "MASS", "odbc", "officer", "pbkrtest", "pryr", "purrr", "Rcpp", "RcppTOML", "reticulate", "rmarkdown", "RPostgres", "sass", "spatial", "stringi", "styler", "survival", "tidyr", "timechange", "vctrs", "vroom", "xgboost", "yaml"))
install.packages(c("BH", "broom", "class", "cli", "colorspace", "curl", "dbplyr", "dplyr", "evaluate", "expm", "fansi", "flextable", "fontawesome", "forcats", "fs", "httpuv", "knitr", "lubridate", "MASS", "odbc", "officer", "pbkrtest", "pryr", "purrr", "Rcpp", "RcppTOML", "reticulate", "rmarkdown", "RPostgres", "sass", "spatial", "stringi", "styler", "survival", "tidyr", "timechange", "vctrs", "vroom", "xgboost", "yaml"))
install.packages(c("BH", "broom", "class", "cli", "colorspace", "curl", "dbplyr", "dplyr", "evaluate", "expm", "fansi", "flextable", "fontawesome", "forcats", "fs", "httpuv", "knitr", "lubridate", "MASS", "odbc", "officer", "pbkrtest", "pryr", "purrr", "Rcpp", "RcppTOML", "reticulate", "rmarkdown", "RPostgres", "sass", "spatial", "stringi", "styler", "survival", "tidyr", "timechange", "vctrs", "vroom", "xgboost", "yaml"))
install.packages(c("BH", "broom", "class", "cli", "colorspace", "curl", "dbplyr", "dplyr", "evaluate", "expm", "fansi", "flextable", "fontawesome", "forcats", "fs", "httpuv", "knitr", "lubridate", "MASS", "odbc", "officer", "pbkrtest", "pryr", "purrr", "Rcpp", "RcppTOML", "reticulate", "rmarkdown", "RPostgres", "sass", "spatial", "stringi", "styler", "survival", "tidyr", "timechange", "vctrs", "vroom", "xgboost", "yaml"))
install.packages(c("BH", "broom", "class", "cli", "colorspace", "curl", "dbplyr", "dplyr", "evaluate", "expm", "fansi", "flextable", "fontawesome", "forcats", "fs", "httpuv", "knitr", "lubridate", "MASS", "odbc", "officer", "pbkrtest", "pryr", "purrr", "Rcpp", "RcppTOML", "reticulate", "rmarkdown", "RPostgres", "sass", "spatial", "stringi", "styler", "survival", "tidyr", "timechange", "vctrs", "vroom", "xgboost", "yaml"))
install.packages(c("BH", "broom", "class", "cli", "colorspace", "curl", "dbplyr", "dplyr", "evaluate", "expm", "fansi", "flextable", "fontawesome", "forcats", "fs", "httpuv", "knitr", "lubridate", "MASS", "odbc", "officer", "pbkrtest", "pryr", "purrr", "Rcpp", "RcppTOML", "reticulate", "rmarkdown", "RPostgres", "sass", "spatial", "stringi", "styler", "survival", "tidyr", "timechange", "vctrs", "vroom", "xgboost", "yaml"))
install.packages(c("BH", "broom", "class", "cli", "colorspace", "curl", "dbplyr", "dplyr", "evaluate", "expm", "fansi", "flextable", "fontawesome", "forcats", "fs", "httpuv", "knitr", "lubridate", "MASS", "odbc", "officer", "pbkrtest", "pryr", "purrr", "Rcpp", "RcppTOML", "reticulate", "rmarkdown", "RPostgres", "sass", "spatial", "stringi", "styler", "survival", "tidyr", "timechange", "vctrs", "vroom", "xgboost", "yaml"))
ttt <- data.frame(gsub("[[:punct:]]", "", donnees, except(".", "-")))
ttt <- data.frame(gsub("[^[:alnum:]\\-\\.\\s]", "", donnees, except(".", "-")))
ttt <- data.frame(gsub("[^[:alnum:]\\-\\.\\s]", "", donnees)))
ttt <- data.frame(gsub("[^[:alnum:]\\-\\.\\s]", "", donnees))
View(ttt)
ttt <- data.frame(gsub("(?!\\.)[[:punct:]]", "", donnees))
ttt <- data.frame(stringr::str_remove_all(donnees$site, "[\\p{P}\\p{S}&&[^.]]"))
View(ttt)
donnees2 <- data.frame(donnees$publication, stringr::str_remove_all(donnees$site, "[\\p{P}\\p{S}&&[^.]]"))
names(donnees2) = c("publication", "site")
## Exportation des données ----
dbWriteTable(conn = con, "publication_sites_comm", donnees2)
dbWriteTable(conn = con, "publication_sites_comm", donnees2)
# chargement des packages ----
library(stringr)
library(tibble)
library(tidytext)
library(RPostgres)
library(lubridate)
library(timechange)
library(urltools)
library(stringr)
library(DBI)
con<-dbConnect(RPostgres::Postgres())
db <- 'SKEPTISCIENCE'  #provide the name of your db
host_db <- 'localhost' # server
db_port <- '5433'  # port DBA
db_user <- 'postgres' # nom utilisateur
db_password <- 'Maroua1912'
con <- dbConnect(RPostgres::Postgres(), dbname = db, host=host_db, port=db_port, user=db_user, password=db_password)
# Test connexion
dbListTables(con)
## Exportation des données ----
dbWriteTable(conn = con, "publication_sites_comm", donnees2)
donnees2 %>% as.numeric(donnees2$publication)
donnees2 %>% as.integer(donnees2$publication)
as.integer(donnees2$publication)
donnees3 <- data.frame(as.integer(donnees2$publication), donnees2$site)
names(donnees3) = c("publication", "site")
## Exportation des données ----
dbWriteTable(conn = con, "publication_sites_comm", donnees3)
## Exportation des données ----
dbWriteTable(conn = con, "publication_sites_comm", donnees3)
View(URL_extract)
View(df)
View(donnees3)
View(list_data)
View(URL_extract)
#### Etape 0 : Transformer le type de données pour plus de facilité/performance dans le traitement ----
URL_var <- as_tibble(data_comm) %>% # Etape 0
#### Etape 1 : Se limiter aux commentaires avec au moins un lien hypertexte  ----
subset(., comm %like% c("%https%","%http%","%www%","%WWW%")) %>% # Etape 1
#### Etape 2 : Recupérer uniquement les commentaires (l'intérêt de garder le subset est de pouvoir selectionner par la suite les disciplines aussi) ----
.$comm # Etape 2
View(data_comm)
data_comm[1,]
View(donnees)
## factor
factor(donnees$site)
## factor
t <- data.frame(donnees$publication, factor(donnees$site))
questionr::describe(t)
names(t) <- c("publication","site")
questionr::describe(t$site)
library(questionr)
install.packages("questionr")
library(questionr)
questionr::describe(t$site)
count(t)
library(dplyr)
